{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import ot\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import copy\n",
    "from math import sqrt\n",
    "\n",
    "from torch.autograd import grad as torch_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "noise_size = 62\n",
    "\n",
    "# Number of training epochs using classical training\n",
    "num_epochs = 20\n",
    "\n",
    "# Number of iterations of Chen training\n",
    "num_Chen_iters = 5000\n",
    "\n",
    "# 'Adam' of 'RMSProp'\n",
    "which_optimizer = 'Adam'\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lrG = 0.000005\n",
    "lrD = 0.00005\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0\n",
    "\n",
    "beta2 = 0.99\n",
    "\n",
    "ngpu = 0\n",
    "\n",
    "# for gradient penalty\n",
    "gp_weight = 10.0\n",
    "\n",
    "batch_size = 7\n",
    "\n",
    "test_batch_size = 16384\n",
    "\n",
    "w_dim = 4\n",
    "\n",
    "a_dim = int(w_dim*(w_dim - 1)//2)\n",
    "\n",
    "# if 1 use GAN1, if 2 use GAN2, etc.\n",
    "which_model = 2\n",
    "\n",
    "# slope for LeakyReLU\n",
    "leakyReLU_slope = 0.2\n",
    "\n",
    "# this gives the option to rum the training process multiple times with differently initialised GANs\n",
    "#num_trials = 1\n",
    "\n",
    "num_tests_for2d = 8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# calculate T and M\n",
    "\n",
    "def generate_signs(n: int):\n",
    "    lst = []\n",
    "    for i in range(2**n):\n",
    "        binary_exp = list(bin(i)[2:])\n",
    "        lst.append((n-len(binary_exp))*[0]+binary_exp)\n",
    "\n",
    "    res = 2*np.array(lst, dtype=float) - np.ones((2**n,n), dtype=float)\n",
    "    return res\n",
    "\n",
    "signs = generate_signs(w_dim)\n",
    "\n",
    "first_dim = []\n",
    "second_dim = []\n",
    "third_dim = []\n",
    "fourth_dim = []\n",
    "values = []\n",
    "M_list = []\n",
    "\n",
    "for s in range(len(signs)):\n",
    "    idx = 0\n",
    "    M_row = []\n",
    "    for i in range(w_dim):\n",
    "        for j in range(i+1,w_dim):\n",
    "            first_dim.append(s)\n",
    "            second_dim.append(idx)\n",
    "            third_dim.append(i)\n",
    "            fourth_dim.append(j)\n",
    "            values.append(-1*signs[s,j].item())\n",
    "            first_dim.append(s)\n",
    "            second_dim.append(idx)\n",
    "            third_dim.append(j)\n",
    "            fourth_dim.append(i)\n",
    "            values.append(signs[s,j].item())\n",
    "            idx+=1\n",
    "            M_row.append(signs[s,j].item() * signs[s,i].item())\n",
    "    M_list.append(M_row)\n",
    "\n",
    "indices = [first_dim,second_dim,third_dim,fourth_dim]\n",
    "T = torch.sparse_coo_tensor(indices=indices,values=values, size = (len(signs),a_dim,w_dim,w_dim)).to_dense()\n",
    "\n",
    "M = torch.tensor(M_list).unsqueeze(1)\n",
    "\n",
    "\n",
    "# A function that takes W, H and B (B is the Levy Area of the Brownian Bridge) and computes A = WTH+MB\n",
    "# where the hell is maribor anyway?\n",
    "def wthmb(w_in: torch.Tensor, h_in: torch.Tensor, b_in: torch.Tensor):\n",
    "    assert w_in.shape == (batch_size,w_dim) and h_in.shape == (batch_size, w_dim) and b_in.shape == (batch_size, a_dim)\n",
    "    _W = w_in.view(1,1,batch_size,w_dim)\n",
    "    _H = h_in.view(batch_size,w_dim,1)\n",
    "    _B = b_in.view(1,batch_size,a_dim)\n",
    "    WT = torch.matmul(_W, T).permute(0,2,1,3)\n",
    "    WTH = torch.matmul(WT, _H).squeeze()\n",
    "    MB = torch.mul(M,_B)\n",
    "    return torch.flatten(WTH + MB, start_dim=0,end_dim=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.],\n",
      "        [17., 18., 19., 20.],\n",
      "        [21., 22., 23., 24.],\n",
      "        [25., 26., 27., 28.]])\n",
      "tensor([[ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.],\n",
      "        [17., 18., 19., 20.],\n",
      "        [21., 22., 23., 24.],\n",
      "        [25., 26., 27., 28.],\n",
      "        [29., 30., 31., 32.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15., 16., 17.],\n",
      "        [18., 19., 20., 21., 22., 23.],\n",
      "        [24., 25., 26., 27., 28., 29.],\n",
      "        [30., 31., 32., 33., 34., 35.],\n",
      "        [36., 37., 38., 39., 40., 41.]])\n"
     ]
    }
   ],
   "source": [
    "h_dim = w_dim\n",
    "W = torch.arange(start = 1, end = w_dim*batch_size+1, dtype = torch.float).view(batch_size,w_dim)\n",
    "H = torch.arange(start = 5, end = h_dim*batch_size+5, dtype = torch.float).view(batch_size,h_dim)\n",
    "B = torch.arange(a_dim*batch_size, dtype=torch.float).view(batch_size,a_dim)\n",
    "print(W)\n",
    "print(H)\n",
    "print(B)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -4.,  -7., -10.,  -1.,  -4.,   1.],\n",
      "        [  2.,  -1.,  -4.,   5.,   2.,   7.],\n",
      "        [  8.,   5.,   2.,  11.,   8.,  13.],\n",
      "        [ 14.,  11.,   8.,  17.,  14.,  19.],\n",
      "        [ 20.,  17.,  14.,  23.,  20.,  25.],\n",
      "        [ 26.,  23.,  20.,  29.,  26.,  31.],\n",
      "        [ 32.,  29.,  26.,  35.,  32.,  37.],\n",
      "        [ -4.,  -7.,  10.,  -1.,   4.,  -1.],\n",
      "        [  2.,  -1.,   4.,   5.,  -2.,  -7.],\n",
      "        [  8.,   5.,  -2.,  11.,  -8., -13.],\n",
      "        [ 14.,  11.,  -8.,  17., -14., -19.],\n",
      "        [ 20.,  17., -14.,  23., -20., -25.],\n",
      "        [ 26.,  23., -20.,  29., -26., -31.],\n",
      "        [ 32.,  29., -26.,  35., -32., -37.],\n",
      "        [ -4.,   7., -10.,   1.,  -4.,  -9.],\n",
      "        [  2.,   1.,  -4.,  -5.,   2., -15.],\n",
      "        [  8.,  -5.,   2., -11.,   8., -21.],\n",
      "        [ 14., -11.,   8., -17.,  14., -27.],\n",
      "        [ 20., -17.,  14., -23.,  20., -33.],\n",
      "        [ 26., -23.,  20., -29.,  26., -39.],\n",
      "        [ 32., -29.,  26., -35.,  32., -45.],\n",
      "        [ -4.,   7.,  10.,   1.,   4.,   9.],\n",
      "        [  2.,   1.,   4.,  -5.,  -2.,  15.],\n",
      "        [  8.,  -5.,  -2., -11.,  -8.,  21.],\n",
      "        [ 14., -11.,  -8., -17., -14.,  27.],\n",
      "        [ 20., -17., -14., -23., -20.,  33.],\n",
      "        [ 26., -23., -20., -29., -26.,  39.],\n",
      "        [ 32., -29., -26., -35., -32.,  45.],\n",
      "        [  4.,  -7., -10.,  -7., -12.,   1.],\n",
      "        [ -2.,  -1.,  -4., -13., -18.,   7.],\n",
      "        [ -8.,   5.,   2., -19., -24.,  13.],\n",
      "        [-14.,  11.,   8., -25., -30.,  19.],\n",
      "        [-20.,  17.,  14., -31., -36.,  25.],\n",
      "        [-26.,  23.,  20., -37., -42.,  31.],\n",
      "        [-32.,  29.,  26., -43., -48.,  37.],\n",
      "        [  4.,  -7.,  10.,  -7.,  12.,  -1.],\n",
      "        [ -2.,  -1.,   4., -13.,  18.,  -7.],\n",
      "        [ -8.,   5.,  -2., -19.,  24., -13.],\n",
      "        [-14.,  11.,  -8., -25.,  30., -19.],\n",
      "        [-20.,  17., -14., -31.,  36., -25.],\n",
      "        [-26.,  23., -20., -37.,  42., -31.],\n",
      "        [-32.,  29., -26., -43.,  48., -37.],\n",
      "        [  4.,   7., -10.,   7., -12.,  -9.],\n",
      "        [ -2.,   1.,  -4.,  13., -18., -15.],\n",
      "        [ -8.,  -5.,   2.,  19., -24., -21.],\n",
      "        [-14., -11.,   8.,  25., -30., -27.],\n",
      "        [-20., -17.,  14.,  31., -36., -33.],\n",
      "        [-26., -23.,  20.,  37., -42., -39.],\n",
      "        [-32., -29.,  26.,  43., -48., -45.],\n",
      "        [  4.,   7.,  10.,   7.,  12.,   9.],\n",
      "        [ -2.,   1.,   4.,  13.,  18.,  15.],\n",
      "        [ -8.,  -5.,  -2.,  19.,  24.,  21.],\n",
      "        [-14., -11.,  -8.,  25.,  30.,  27.],\n",
      "        [-20., -17., -14.,  31.,  36.,  33.],\n",
      "        [-26., -23., -20.,  37.,  42.,  39.],\n",
      "        [-32., -29., -26.,  43.,  48.,  45.],\n",
      "        [ -4.,  -9., -14.,  -1.,  -4.,   1.],\n",
      "        [-10., -15., -20.,   5.,   2.,   7.],\n",
      "        [-16., -21., -26.,  11.,   8.,  13.],\n",
      "        [-22., -27., -32.,  17.,  14.,  19.],\n",
      "        [-28., -33., -38.,  23.,  20.,  25.],\n",
      "        [-34., -39., -44.,  29.,  26.,  31.],\n",
      "        [-40., -45., -50.,  35.,  32.,  37.],\n",
      "        [ -4.,  -9.,  14.,  -1.,   4.,  -1.],\n",
      "        [-10., -15.,  20.,   5.,  -2.,  -7.],\n",
      "        [-16., -21.,  26.,  11.,  -8., -13.],\n",
      "        [-22., -27.,  32.,  17., -14., -19.],\n",
      "        [-28., -33.,  38.,  23., -20., -25.],\n",
      "        [-34., -39.,  44.,  29., -26., -31.],\n",
      "        [-40., -45.,  50.,  35., -32., -37.],\n",
      "        [ -4.,   9., -14.,   1.,  -4.,  -9.],\n",
      "        [-10.,  15., -20.,  -5.,   2., -15.],\n",
      "        [-16.,  21., -26., -11.,   8., -21.],\n",
      "        [-22.,  27., -32., -17.,  14., -27.],\n",
      "        [-28.,  33., -38., -23.,  20., -33.],\n",
      "        [-34.,  39., -44., -29.,  26., -39.],\n",
      "        [-40.,  45., -50., -35.,  32., -45.],\n",
      "        [ -4.,   9.,  14.,   1.,   4.,   9.],\n",
      "        [-10.,  15.,  20.,  -5.,  -2.,  15.],\n",
      "        [-16.,  21.,  26., -11.,  -8.,  21.],\n",
      "        [-22.,  27.,  32., -17., -14.,  27.],\n",
      "        [-28.,  33.,  38., -23., -20.,  33.],\n",
      "        [-34.,  39.,  44., -29., -26.,  39.],\n",
      "        [-40.,  45.,  50., -35., -32.,  45.],\n",
      "        [  4.,  -9., -14.,  -7., -12.,   1.],\n",
      "        [ 10., -15., -20., -13., -18.,   7.],\n",
      "        [ 16., -21., -26., -19., -24.,  13.],\n",
      "        [ 22., -27., -32., -25., -30.,  19.],\n",
      "        [ 28., -33., -38., -31., -36.,  25.],\n",
      "        [ 34., -39., -44., -37., -42.,  31.],\n",
      "        [ 40., -45., -50., -43., -48.,  37.],\n",
      "        [  4.,  -9.,  14.,  -7.,  12.,  -1.],\n",
      "        [ 10., -15.,  20., -13.,  18.,  -7.],\n",
      "        [ 16., -21.,  26., -19.,  24., -13.],\n",
      "        [ 22., -27.,  32., -25.,  30., -19.],\n",
      "        [ 28., -33.,  38., -31.,  36., -25.],\n",
      "        [ 34., -39.,  44., -37.,  42., -31.],\n",
      "        [ 40., -45.,  50., -43.,  48., -37.],\n",
      "        [  4.,   9., -14.,   7., -12.,  -9.],\n",
      "        [ 10.,  15., -20.,  13., -18., -15.],\n",
      "        [ 16.,  21., -26.,  19., -24., -21.],\n",
      "        [ 22.,  27., -32.,  25., -30., -27.],\n",
      "        [ 28.,  33., -38.,  31., -36., -33.],\n",
      "        [ 34.,  39., -44.,  37., -42., -39.],\n",
      "        [ 40.,  45., -50.,  43., -48., -45.],\n",
      "        [  4.,   9.,  14.,   7.,  12.,   9.],\n",
      "        [ 10.,  15.,  20.,  13.,  18.,  15.],\n",
      "        [ 16.,  21.,  26.,  19.,  24.,  21.],\n",
      "        [ 22.,  27.,  32.,  25.,  30.,  27.],\n",
      "        [ 28.,  33.,  38.,  31.,  36.,  33.],\n",
      "        [ 34.,  39.,  44.,  37.,  42.,  39.],\n",
      "        [ 40.,  45.,  50.,  43.,  48.,  45.]])\n"
     ]
    }
   ],
   "source": [
    "A = wthmb(W,H,B)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "blip = torch.tensor([[1,2,3],[4,5,6]])\n",
    "blup = blip.repeat(4,1)\n",
    "print(blup)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class symGenerator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(symGenerator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim+noise_size,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,w_dim+a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        w = input[:,noise_size:w_dim+noise_size]\n",
    "        x = self.main(input)\n",
    "        h = x[:,:w_dim]\n",
    "        b = x[:,w_dim:w_dim+a_dim]\n",
    "        return wthmb(w,h,b)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class HsymGenerator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HsymGenerator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim +noise_size,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        w = input[:,noise_size:w_dim+noise_size]\n",
    "        bsz = input.shape[0]\n",
    "        h = sqrt(1/12) * torch.randn((bsz,w_dim), dtype=torch.float)\n",
    "        noise = input[:,:noise_size]\n",
    "        x = torch.cat((noise,h),dim=1)\n",
    "        b = self.main(x)\n",
    "        return wthmb(w,h,b)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
