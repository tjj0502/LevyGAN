# LevyGAN

This is a work in progress, a more detailed description and further test results will be available later.

In short I am using a WGAN-GP to generate samples of Levy-area (see Chen_relation.pdf for a definition of Levy area in 2D or chapter 7 of [2] for the general formula).

The motivation for this is Milstein’s method for approximating SDEs, which has strong convergence of O(h) (as opposed to Euler-Murayama with O(h^1/2)), but requires access to samples of Levy-area. The issue is that for W of dimension 3 or higher no one has so far found sufficiently fast methods of generating Levy-area for this to be worth it. Here GANs are useful since they are good at learning vaguely bell-shaped distributions, and while they tend to be hard to train, once they are, they are fast to evaluate. But I only need to train a GAN once to subsequently use it in as many applications of Milstein’s method as I want. Furthermore slow training isn’t an issue, since generating the training samples classically takes a lot of time anyway. In particular I use conditional GANs, where I can input a particular increment of W and it generates samples of Levy-area conditional on that. There is still some hyperparameter optimization and general tweaking to be done, but even in its unfinished form my GAN is significantly faster than state-of-the-art classical methods, while offering the same precision.

Two methods of interest that I use are Chen training and flipping Brownian bridges.

Chen training is a way to train the GAN without providing any data at all. It uses the Chen relation for Levy-Area (see Chen_relation.pdf) to “improve” some of the data generated by the generator, which is then passed to the discriminator as the “true” data and the unimproved data is passed as the “fake”. Unsurprisingly training is slower this way, but the GAN nevertheless successfully learns the correct distribution without any data input whatsoever (note that after training we do not use the Chen relation to improve outputs any more, so it genuinely does learn the correct distribution). This is especially effective if the net is first trained on some classically generated low precision data and then trained further with this approach.

With flipping Brownian bridges I mean the following. Instead of generating the entire Levy-area, I generate the Levy-area of the Brownian bridges and then mirror them in all 2^d directions (where d=dim(W)) and randomly select one of those (or for training just take all of them to get a higher sample size). Then I use Theorem 7.0.13 from [2] to produce the entire Levy-area. This ensures that the samples are unbiased, which gives theoretical convergence guarantees for Milstein’s method.

In the file classical_sample_generator I use the package LevyArea from [1].

References:
1. F. Kastner and A. Rößler. LevyArea.jl. 2022. doi: 10.5281/zenodo.5883748. url: https://github.com/stochastics-uni-luebeck/LevyArea.jl
2. Foster, J. M. Numerical Approximations for Stochastic Differential Equations. University of Oxford, 2020.
3. J. M. C. Clark and R. J. Cameron. The maximum rate of convergence of discrete approximations for Stochastic differential equations. in Stochastic Differential Systems Filtering and Control, ed. by Grigelionis (Springer, Berlin), 1980.
4. A. S. Dickinson. Optimal Approximation of the Second Iterated Integral of Brownian Motion. Stochastic Analysis and Applications, 25(5):1109–1128, 2007.
