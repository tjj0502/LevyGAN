# LevyGAN

This is a work in progress, a more detailed description and further test results will be available later.

In short I am using a WGAN-GP to generate samples of Levy-area (see Chen_relation.pdf for a definition of Levy area in 2D or chapter 7 of [1] for the general formula).

The motivation for this is Milstein’s method for approximating SDEs, which has strong convergence of O(h) (as opposed to Euler-Murayama with O(h^1/2)), but requires access to samples of Levy-area. The issue is that for W of dimension 3 or higher no one has so far found sufficiently fast methods of generating Levy-area for this to be worth it. Here GANs are useful since they are good at learning vaguely bell-shaped distributions, and while they tend to be hard to train, once they are, they are fast to evaluate. But I only need to train a GAN once to subsequently use it in as many applications of Milstein’s method as I want. Furthermore slow training isn’t an issue, since generating the training samples classically takes a lot of time anyway. In particular I use conditional GANs, where I can input a particular increment of W and it generates samples of Levy-area conditional on that. There is still some hyperparameter optimization and general tweaking to be done, but even in its unfinished form my GAN is significantly faster than state-of-the-art classical methods, while offering the same precision.

Two methods of interest that I use are Chen training and flipping Brownian bridges.

Chen training is a way to train the GAN without providing any data at all. It uses the Chen relation for Levy-Area (see Chen_relation.pdf) to “improve” some of the data generated by the generator, which is then passed to the discriminator as the “true” data and the unimproved data is passed as the “fake”. Unsurprisingly training is slower this way, but the GAN nevertheless successfully learns the correct distribution without any data input whatsoever (note that after training we do not use the Chen relation to improve outputs any more, so it genuinely does learn the correct distribution). This is especially effective if the net is first trained on some classically generated low precision data and then trained further with this approach.

With flipping Brownian bridges I mean the following. Instead of generating the entire Levy-area, I generate the Levy-area of the Brownian bridges and then mirror them in all 2^d directions (where d=dim(W)) and randomly select one of those (or for training just take all of them to get a higher sample size). Then I use Theorem 7.0.13 from [1] to produce the entire Levy-area. This ensures that the samples are unbiased, which gives theoretical convergence guarantees for Milstein’s method.

In the file classical_sample_generator I use the package LevyArea from [8].

References:
1. Foster, J. M. Numerical Approximations for Stochastic Differential Equations, 2020.
2. Andrew S. Dickinson. Optimal Approximation of the Second Iterated Integral of Brownian Motion, Stochastic Analysis and applications, 25:5, 1109-1128, DOI: 10.1080/07362990701540592, 2007.
3. Foster, J., & Habermann, K. (2021). Brownian bridge expansions for L\'evy area approximations and particular values of the Riemann zeta function. arXiv preprint arXiv:2102.10095.
4. Mackevičius, V.. Introduction to Stochastic Analysis : Integrals and Differential Equations. ISTE Ltd ; John Wiley, 2011.
5. Flint, Guy and Lyons, T. J. Pathwise approximation of SDEs by coupling piecewise abelian rough paths. arXiv: Probability, 2015.
6. Gaines, J. G. and Lyons, T. J. Random generation of stochastic area integrals. SIAM Journal on Applied Mathematics, 54(4):1132–1146, 1994.
7. Gulrajani, Ishaan, et al. Improved training of wasserstein gans. Advances in neural information processing systems 30, 2017.
8. Kastner, Felix, and Andreas Rößler. "An Analysis of Approximation Algorithms for Iterated Stochastic Integrals and a Julia and MATLAB Simulation Toolbox." arXiv preprint arXiv:2201.08424, 2022.
9. Lyons, T. J., et al. Differential Equations Driven by Rough Paths Ecole D'eté De probabilités De Saint-Flour XXXIV-2004. Springer Berlin Heidelberg, 2007. 
10. Chen, K. T. Integration of paths, geometric invariants and a generalized Baker-Hausdorff formula. Annals of Mathematics, pages 163–178, 1957.
11. Milstein, G. N. and Tretyakov, M. V. Stochastic Numerics for Mathematical Physics. Springer Berlin Heidelberg, 2004.
12. Clark, J. Martin C. and Russell J. Cameron. The maximum rate of convergence of discrete approximations. 1980.
