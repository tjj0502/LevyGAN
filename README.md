# LevyGAN

This is a work in progress, a more detailed description and further test results will be available later.

I have presented this research at the Rough Path Theory Seminar, you can find the slides at the link below:
https://docs.google.com/presentation/d/1A4vU1GdRtKQ6DYr1jp3IxLyiqeP5exLBZhLOLna9zgo/edit?usp=sharing

I will soon also write a dissertation on this, and will upload it here as soon as I can.

Here is a quick breakdown of the relevant .py files and what you can find where (for a conceptual walkthrough, look further down):
- TheGAN.py: training, testing, measuring errors, logging, drawing graphs, saving dictionaries...
- nets.py: neural nets of various depths and widths
- Generator.py, Discriminator.py: the other logic in the models built around the neural nets, most of it related to Bridge-Flipping (see below)
- aux_functions.py: random functions like those for computing some statistical properties of distributions, most of it just there for convenience
- bayesian_hyperparam_opt.py: exactly what you think
- precisions.py: tries various methods, computes their precision and runtime (used for final results)
- the .ipynb files for testing stuff on the fly, essentially just scrap paper
- the rest is not very important


In short I am using a WGAN-GP to generate samples of Levy-area (see Chen_relation.pdf for a definition of Levy area in 2D or chapter 7 of [1] for the general formula).

The motivation for this is Milstein’s method for approximating SDEs, which has strong convergence of O(h) (as opposed to Euler-Murayama with O(h^1/2)), but requires access to samples of Levy-area. The issue is that for W of dimension 3 or higher no one has so far found sufficiently fast methods of generating Levy-area for this to be worth it. Here GANs are useful since they are good at learning vaguely bell-shaped distributions, and while they tend to be hard to train, once they are, they are fast to evaluate. But I only need to train a GAN once to subsequently use it in as many applications of Milstein’s method as I want. Furthermore slow training isn’t an issue, since generating the training samples classically takes a lot of time anyway. In particular I use conditional GANs, where I can input a particular increment of W and it generates samples of Levy-area conditional on that. There is still some hyperparameter optimization and general tweaking to be done, but even in its unfinished form my GAN is significantly faster than state-of-the-art classical methods, while offering the same precision.

Two methods of interest that I use are Chen training and flipping Brownian bridges.

Chen training is a way to train the GAN without providing any data at all. It uses the Chen relation for Levy-Area (see Chen_relation.pdf) to “improve” some of the data generated by the generator, which is then passed to the discriminator as the “true” data and the unimproved data is passed as the “fake”. Unsurprisingly training is slower this way, but the GAN nevertheless successfully learns the correct distribution without any data input whatsoever (note that after training we do not use the Chen relation to improve outputs any more, so it genuinely does learn the correct distribution). This is especially effective if the net is first trained on some classically generated low precision data and then trained further with this approach. Below are two slides from my presentation, showing the schematics of Chen training. You can find the relevant code in aux_functions.chen_combine, and TheGAN.chen_train.

![Screenshot from 2023-01-04 21-16-50](https://user-images.githubusercontent.com/66168650/210654465-88422849-8e5a-4920-8358-9d90539cd74f.png)
![Screenshot from 2023-01-04 21-16-58](https://user-images.githubusercontent.com/66168650/210654541-8aa69896-bf77-40d0-b020-5eefe07d0985.png)

With flipping Brownian bridges I mean the following. Instead of generating the entire Levy-area, I generate the Levy-area of the Brownian bridges and then mirror them in all 2^d directions (where d=dim(W)) and randomly select one of those (or for training just take all of them to get a higher sample size). Then I use Theorem 7.0.13 from [1] to produce the entire Levy-area. This ensures that the samples are unbiased, which gives theoretical convergence guarantees for Milstein’s method. For the code check out aux_compute_wth and aux_compute_wthmb from aux_functions.py. Similar functions also appear in Generator.py. Here are the relevant equations and a schematic of this:

![Screenshot from 2023-01-04 21-17-53](https://user-images.githubusercontent.com/66168650/210654583-c2023ef4-4b13-4ef8-9cf8-fa15615cb487.png)
![Screenshot from 2023-01-04 21-18-25](https://user-images.githubusercontent.com/66168650/210654598-807f2559-5e1f-4ccd-b7e1-a1500d11ba6f.png)

In the file classical_sample_generator.jl I use the package LevyArea from [8].

References:
1. Foster, J. M. Numerical Approximations for Stochastic Differential Equations, 2020.
2. Andrew S. Dickinson. Optimal Approximation of the Second Iterated Integral of Brownian Motion, Stochastic Analysis and applications, 25:5, 1109-1128, DOI: 10.1080/07362990701540592, 2007.
3. Foster, J., & Habermann, K. (2021). Brownian bridge expansions for L\'evy area approximations and particular values of the Riemann zeta function. arXiv preprint arXiv:2102.10095.
4. Mackevičius, V.. Introduction to Stochastic Analysis : Integrals and Differential Equations. ISTE Ltd ; John Wiley, 2011.
5. Flint, Guy and Lyons, T. J. Pathwise approximation of SDEs by coupling piecewise abelian rough paths. arXiv: Probability, 2015.
6. Gaines, J. G. and Lyons, T. J. Random generation of stochastic area integrals. SIAM Journal on Applied Mathematics, 54(4):1132–1146, 1994.
7. Gulrajani, Ishaan, et al. Improved training of wasserstein gans. Advances in neural information processing systems 30, 2017.
8. Kastner, Felix, and Andreas Rößler. "An Analysis of Approximation Algorithms for Iterated Stochastic Integrals and a Julia and MATLAB Simulation Toolbox." arXiv preprint arXiv:2201.08424, 2022.
9. Lyons, T. J., et al. Differential Equations Driven by Rough Paths Ecole D'eté De probabilités De Saint-Flour XXXIV-2004. Springer Berlin Heidelberg, 2007. 
10. Chen, K. T. Integration of paths, geometric invariants and a generalized Baker-Hausdorff formula. Annals of Mathematics, pages 163–178, 1957.
11. Milstein, G. N. and Tretyakov, M. V. Stochastic Numerics for Mathematical Physics. Springer Berlin Heidelberg, 2004.
12. Clark, J. Martin C. and Russell J. Cameron. The maximum rate of convergence of discrete approximations. 1980.
