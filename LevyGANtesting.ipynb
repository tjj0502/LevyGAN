{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import ot\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import copy\n",
    "from math import sqrt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "noise_size = 24\n",
    "\n",
    "# Number of training epochs using classical training\n",
    "num_epochs = 5\n",
    "\n",
    "# Number of iterations of Chen training\n",
    "num_Chen_iters = 0\n",
    "\n",
    "# 'Adam' of 'RMSProp'\n",
    "which_optimizer = 'Adam'\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0001\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "ngpu = 0\n",
    "\n",
    "# To keep the criterion Lipschitz\n",
    "weight_cliping_limit = 0.01\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "test_batch_size = 65536\n",
    "\n",
    "w_dim = 3\n",
    "\n",
    "a_dim = int(w_dim*(w_dim - 1)//2)\n",
    "\n",
    "# if 1 use GAN1, if 2 use GAN2, etc.\n",
    "which_model = 1\n",
    "\n",
    "# slope for LeakyReLU\n",
    "leakyReLU_slope = 0.2\n",
    "\n",
    "# this gives the option to rum the training process multiple times with differently initialised GANs\n",
    "num_trials = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# CHEN RELATION\n",
    "# Levy-area satisfies a version of the Chen relation (see Chen_relation.pdf) and is the unique distribution which satisfies this version of the relation\n",
    "\n",
    "def chen_combine(w_a_in: torch.TensorType):\n",
    "    # the batch dimension of the inputs will be quartered\n",
    "    out_size = w_a_in.size(0) // 2\n",
    "    assert 2 * out_size == w_a_in.size(0)\n",
    "    assert w_a_in.size(1) == w_dim + a_dim\n",
    "\n",
    "    # w_0_s is from 0 to t/2 and w_s_t is from t/2 to t\n",
    "    w_0_s, w_s_t = w_a_in.chunk(2)\n",
    "    result = torch.clone(w_0_s + w_s_t)\n",
    "    result[:, :w_dim] = sqrt(0.5) * result[:, :w_dim]\n",
    "    result[:, w_dim:(w_dim + a_dim)] = 0.5 * result[:, w_dim:(w_dim + a_dim)]\n",
    "\n",
    "    idx = w_dim\n",
    "    for k in range(w_dim - 1):\n",
    "        for l in range(k + 1, w_dim):\n",
    "            correction_term = 0.25 * (w_0_s[:, k] * w_s_t[:, l] - w_0_s[:, l] * w_s_t[:, k])\n",
    "            result[:, idx] += correction_term\n",
    "            idx += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# prints the 2-Wasserstein distances (in each of the Levy-area dimensions) between the input and chen_combine(chen_combine(input))\n",
    "# The idea behind this is that Levy-area is the unique distribution which is close to chen_combine of itself\n",
    "# Indeed this is experimentally confirmed in test.ipynb\n",
    "\n",
    "def chen_error_2step(w_a_in: torch.TensorType):\n",
    "    combined_data = chen_combine(w_a_in)\n",
    "    combined_data = chen_combine(combined_data)\n",
    "    return [sqrt(ot.wasserstein_1d(combined_data[:, w_dim + i], w_a_in[:, w_dim + i], p=2)) for i in range(a_dim)]\n",
    "\n",
    "\n",
    "# create dataloader for samples\n",
    "\n",
    "def row_processer(row):\n",
    "    return np.array(row, dtype=np.float32)\n",
    "\n",
    "\n",
    "filename = f\"samples/samples_{w_dim}-dim.csv\"\n",
    "datapipe = dp.iter.FileOpener([filename], mode='b')\n",
    "datapipe = datapipe.parse_csv(delimiter=',')\n",
    "datapipe = datapipe.map(row_processer)\n",
    "dataloader = DataLoader(dataset=datapipe, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    if classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# GAN 1\n",
    "\n",
    "class Generator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + noise_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "\n",
    "# GAN 2\n",
    "\n",
    "class Generator2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + noise_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "# GAN 3\n",
    "\n",
    "class Generator3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator3, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + noise_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator3, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "# A fixed W increment for testing purposes\n",
    "W_fixed: torch.Tensor = torch.tensor([1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7])\n",
    "\n",
    "# function that lists which pairs of W_fixed features are used for each dimension of a_dim\n",
    "def list_pairs(m):\n",
    "    fixed_w_list = [1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7]\n",
    "    lst =[]\n",
    "    for k in range(m):\n",
    "        for l in range(k+1,m):\n",
    "            lst.append((fixed_w_list[k],fixed_w_list[l]))\n",
    "\n",
    "    return lst\n",
    "\n",
    "W_fixed = W_fixed[:w_dim].unsqueeze(1).transpose(1,0)\n",
    "W_fixed = W_fixed.expand((test_batch_size,w_dim))\n",
    "\n",
    "# Load \"true\" samples generated from this fixed W increment\n",
    "test_filename = f\"samples/fixed_samples_{w_dim}-dim.csv\"\n",
    "A_fixed_true = np.genfromtxt(test_filename,dtype=float,delimiter=',',)\n",
    "A_fixed_true = A_fixed_true[:,w_dim:(w_dim+a_dim)]\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial: 0/1,  epoch: 0/5, iter: 0,\n",
      " errors: ['0.11235', '0.21900', '0.19593']\n",
      "Saved parameters\n",
      "trial: 0/1,  epoch: 0/5, iter: 100,\n",
      " errors: ['0.17007', '0.07912', '0.16047']\n",
      "Saved parameters\n",
      "trial: 0/1,  epoch: 0/5, iter: 200,\n",
      " errors: ['0.09119', '0.05050', '0.15665']\n",
      "Saved parameters\n",
      "trial: 0/1,  epoch: 0/5, iter: 300,\n",
      " errors: ['0.11658', '0.06649', '0.14959']\n",
      "trial: 0/1,  epoch: 0/5, iter: 400,\n",
      " errors: ['0.09188', '0.08816', '0.14740']\n",
      "trial: 0/1,  epoch: 0/5, iter: 500,\n",
      " errors: ['0.13068', '0.09942', '0.15854']\n",
      "trial: 0/1,  epoch: 0/5, iter: 600,\n",
      " errors: ['0.13845', '0.11052', '0.15226']\n",
      "trial: 0/1,  epoch: 0/5, iter: 700,\n",
      " errors: ['0.14787', '0.11502', '0.18959']\n",
      "trial: 0/1,  epoch: 0/5, iter: 800,\n",
      " errors: ['0.15430', '0.11481', '0.16694']\n",
      "trial: 0/1,  epoch: 0/5, iter: 900,\n",
      " errors: ['0.12659', '0.11334', '0.16400']\n",
      "trial: 0/1,  epoch: 0/5, iter: 1000,\n",
      " errors: ['0.13571', '0.12338', '0.16405']\n",
      "trial: 0/1,  epoch: 0/5, iter: 1100,\n",
      " errors: ['0.11894', '0.11814', '0.16241']\n",
      "trial: 0/1,  epoch: 0/5, iter: 1200,\n",
      " errors: ['0.10912', '0.12402', '0.15508']\n",
      "trial: 0/1,  epoch: 0/5, iter: 1300,\n",
      " errors: ['0.10925', '0.13680', '0.14768']\n",
      "trial: 0/1,  epoch: 0/5, iter: 1400,\n",
      " errors: ['0.10158', '0.14060', '0.16028']\n",
      "trial: 0/1,  epoch: 0/5, iter: 1500,\n",
      " errors: ['0.10026', '0.14718', '0.15321']\n",
      "trial: 0/1,  epoch: 0/5, iter: 1600,\n",
      " errors: ['0.09846', '0.14818', '0.16390']\n",
      "trial: 0/1,  epoch: 0/5, iter: 1700,\n",
      " errors: ['0.10045', '0.15863', '0.16175']\n",
      "trial: 0/1,  epoch: 0/5, iter: 1800,\n",
      " errors: ['0.10666', '0.13587', '0.16923']\n",
      "trial: 0/1,  epoch: 0/5, iter: 1900,\n",
      " errors: ['0.10110', '0.18685', '0.15895']\n",
      "trial: 0/1,  epoch: 0/5, iter: 2000,\n",
      " errors: ['0.09763', '0.14933', '0.15156']\n",
      "trial: 0/1,  epoch: 1/5, iter: 2100,\n",
      " errors: ['0.09934', '0.13552', '0.16879']\n",
      "trial: 0/1,  epoch: 1/5, iter: 2200,\n",
      " errors: ['0.09620', '0.14550', '0.15781']\n",
      "trial: 0/1,  epoch: 1/5, iter: 2300,\n",
      " errors: ['0.09634', '0.09158', '0.16184']\n",
      "trial: 0/1,  epoch: 1/5, iter: 2400,\n",
      " errors: ['0.10266', '0.11261', '0.16030']\n",
      "trial: 0/1,  epoch: 1/5, iter: 2500,\n",
      " errors: ['0.11347', '0.14127', '0.16140']\n",
      "trial: 0/1,  epoch: 1/5, iter: 2600,\n",
      " errors: ['0.10068', '0.14099', '0.16094']\n",
      "trial: 0/1,  epoch: 1/5, iter: 2700,\n",
      " errors: ['0.10901', '0.10913', '0.15157']\n",
      "trial: 0/1,  epoch: 1/5, iter: 2800,\n",
      " errors: ['0.09621', '0.09299', '0.17295']\n",
      "trial: 0/1,  epoch: 1/5, iter: 2900,\n",
      " errors: ['0.08898', '0.13634', '0.15545']\n",
      "trial: 0/1,  epoch: 1/5, iter: 3000,\n",
      " errors: ['0.08518', '0.10196', '0.16315']\n",
      "trial: 0/1,  epoch: 1/5, iter: 3100,\n",
      " errors: ['0.10396', '0.10085', '0.14392']\n",
      "trial: 0/1,  epoch: 1/5, iter: 3200,\n",
      " errors: ['0.10915', '0.10040', '0.19374']\n",
      "trial: 0/1,  epoch: 1/5, iter: 3300,\n",
      " errors: ['0.10901', '0.13091', '0.14785']\n",
      "trial: 0/1,  epoch: 1/5, iter: 3400,\n",
      " errors: ['0.09449', '0.12563', '0.15018']\n",
      "trial: 0/1,  epoch: 1/5, iter: 3500,\n",
      " errors: ['0.09636', '0.11915', '0.14192']\n",
      "trial: 0/1,  epoch: 1/5, iter: 3600,\n",
      " errors: ['0.09411', '0.09816', '0.15133']\n",
      "trial: 0/1,  epoch: 1/5, iter: 3700,\n",
      " errors: ['0.09744', '0.11043', '0.15552']\n",
      "trial: 0/1,  epoch: 1/5, iter: 3800,\n",
      " errors: ['0.09617', '0.09389', '0.14470']\n",
      "trial: 0/1,  epoch: 1/5, iter: 3900,\n",
      " errors: ['0.09809', '0.09821', '0.15304']\n",
      "trial: 0/1,  epoch: 1/5, iter: 4000,\n",
      " errors: ['0.09815', '0.09673', '0.14779']\n",
      "trial: 0/1,  epoch: 2/5, iter: 4100,\n",
      " errors: ['0.10476', '0.10626', '0.14385']\n",
      "trial: 0/1,  epoch: 2/5, iter: 4200,\n",
      " errors: ['0.11497', '0.14123', '0.14043']\n",
      "trial: 0/1,  epoch: 2/5, iter: 4300,\n",
      " errors: ['0.10829', '0.13116', '0.14806']\n",
      "trial: 0/1,  epoch: 2/5, iter: 4400,\n",
      " errors: ['0.12623', '0.13778', '0.14672']\n",
      "trial: 0/1,  epoch: 2/5, iter: 4500,\n",
      " errors: ['0.11520', '0.12386', '0.14804']\n",
      "trial: 0/1,  epoch: 2/5, iter: 4600,\n",
      " errors: ['0.13001', '0.14209', '0.13522']\n",
      "trial: 0/1,  epoch: 2/5, iter: 4700,\n",
      " errors: ['0.10402', '0.10617', '0.14184']\n",
      "trial: 0/1,  epoch: 2/5, iter: 4800,\n",
      " errors: ['0.11010', '0.13417', '0.13838']\n",
      "trial: 0/1,  epoch: 2/5, iter: 4900,\n",
      " errors: ['0.10634', '0.11975', '0.14140']\n",
      "trial: 0/1,  epoch: 2/5, iter: 5000,\n",
      " errors: ['0.11590', '0.13837', '0.13715']\n",
      "trial: 0/1,  epoch: 2/5, iter: 5100,\n",
      " errors: ['0.11717', '0.13559', '0.14872']\n",
      "trial: 0/1,  epoch: 2/5, iter: 5200,\n",
      " errors: ['0.12402', '0.15249', '0.14969']\n",
      "trial: 0/1,  epoch: 2/5, iter: 5300,\n",
      " errors: ['0.11669', '0.14901', '0.14806']\n",
      "trial: 0/1,  epoch: 2/5, iter: 5400,\n",
      " errors: ['0.10600', '0.15028', '0.15500']\n",
      "trial: 0/1,  epoch: 2/5, iter: 5500,\n",
      " errors: ['0.11390', '0.15288', '0.14702']\n",
      "trial: 0/1,  epoch: 2/5, iter: 5600,\n",
      " errors: ['0.13245', '0.11855', '0.14610']\n",
      "trial: 0/1,  epoch: 2/5, iter: 5700,\n",
      " errors: ['0.12189', '0.11855', '0.14694']\n",
      "trial: 0/1,  epoch: 2/5, iter: 5800,\n",
      " errors: ['0.11297', '0.13527', '0.14591']\n",
      "trial: 0/1,  epoch: 2/5, iter: 5900,\n",
      " errors: ['0.10882', '0.14296', '0.13259']\n",
      "trial: 0/1,  epoch: 2/5, iter: 6000,\n",
      " errors: ['0.10487', '0.13946', '0.14699']\n",
      "trial: 0/1,  epoch: 2/5, iter: 6100,\n",
      " errors: ['0.11144', '0.14147', '0.14056']\n",
      "trial: 0/1,  epoch: 3/5, iter: 6200,\n",
      " errors: ['0.12211', '0.13837', '0.15709']\n",
      "trial: 0/1,  epoch: 3/5, iter: 6300,\n",
      " errors: ['0.12006', '0.13707', '0.15843']\n",
      "trial: 0/1,  epoch: 3/5, iter: 6400,\n",
      " errors: ['0.12221', '0.15437', '0.16607']\n",
      "trial: 0/1,  epoch: 3/5, iter: 6500,\n",
      " errors: ['0.12344', '0.16072', '0.13940']\n",
      "trial: 0/1,  epoch: 3/5, iter: 6600,\n",
      " errors: ['0.12864', '0.15783', '0.16437']\n",
      "trial: 0/1,  epoch: 3/5, iter: 6700,\n",
      " errors: ['0.12755', '0.16487', '0.14828']\n",
      "trial: 0/1,  epoch: 3/5, iter: 6800,\n",
      " errors: ['0.12752', '0.16255', '0.15762']\n",
      "trial: 0/1,  epoch: 3/5, iter: 6900,\n",
      " errors: ['0.13647', '0.13931', '0.14744']\n",
      "trial: 0/1,  epoch: 3/5, iter: 7000,\n",
      " errors: ['0.11713', '0.16447', '0.17073']\n",
      "trial: 0/1,  epoch: 3/5, iter: 7100,\n",
      " errors: ['0.11324', '0.16848', '0.14243']\n",
      "trial: 0/1,  epoch: 3/5, iter: 7200,\n",
      " errors: ['0.10796', '0.16381', '0.14506']\n",
      "trial: 0/1,  epoch: 3/5, iter: 7300,\n",
      " errors: ['0.13525', '0.15316', '0.16383']\n",
      "trial: 0/1,  epoch: 3/5, iter: 7400,\n",
      " errors: ['0.14123', '0.14932', '0.14361']\n",
      "trial: 0/1,  epoch: 3/5, iter: 7500,\n",
      " errors: ['0.12969', '0.16182', '0.16555']\n",
      "trial: 0/1,  epoch: 3/5, iter: 7600,\n",
      " errors: ['0.13084', '0.14210', '0.15730']\n",
      "trial: 0/1,  epoch: 3/5, iter: 7700,\n",
      " errors: ['0.10274', '0.13443', '0.16354']\n",
      "trial: 0/1,  epoch: 3/5, iter: 7800,\n",
      " errors: ['0.12039', '0.15890', '0.15898']\n",
      "trial: 0/1,  epoch: 3/5, iter: 7900,\n",
      " errors: ['0.12092', '0.17177', '0.14668']\n",
      "trial: 0/1,  epoch: 3/5, iter: 8000,\n",
      " errors: ['0.11232', '0.14387', '0.16877']\n",
      "trial: 0/1,  epoch: 3/5, iter: 8100,\n",
      " errors: ['0.11130', '0.13434', '0.15515']\n",
      "trial: 0/1,  epoch: 4/5, iter: 8200,\n",
      " errors: ['0.12054', '0.15406', '0.14885']\n",
      "trial: 0/1,  epoch: 4/5, iter: 8300,\n",
      " errors: ['0.10101', '0.15561', '0.14786']\n",
      "trial: 0/1,  epoch: 4/5, iter: 8400,\n",
      " errors: ['0.11575', '0.18300', '0.16212']\n",
      "trial: 0/1,  epoch: 4/5, iter: 8500,\n",
      " errors: ['0.11566', '0.17555', '0.15458']\n",
      "trial: 0/1,  epoch: 4/5, iter: 8600,\n",
      " errors: ['0.10533', '0.14663', '0.17042']\n",
      "trial: 0/1,  epoch: 4/5, iter: 8700,\n",
      " errors: ['0.09779', '0.13393', '0.15921']\n",
      "trial: 0/1,  epoch: 4/5, iter: 8800,\n",
      " errors: ['0.11645', '0.14000', '0.17026']\n",
      "trial: 0/1,  epoch: 4/5, iter: 8900,\n",
      " errors: ['0.11500', '0.17501', '0.15272']\n",
      "trial: 0/1,  epoch: 4/5, iter: 9000,\n",
      " errors: ['0.11383', '0.17386', '0.15441']\n",
      "trial: 0/1,  epoch: 4/5, iter: 9100,\n",
      " errors: ['0.10603', '0.15979', '0.15283']\n",
      "trial: 0/1,  epoch: 4/5, iter: 9200,\n",
      " errors: ['0.11169', '0.13197', '0.15086']\n",
      "trial: 0/1,  epoch: 4/5, iter: 9300,\n",
      " errors: ['0.09338', '0.14371', '0.14661']\n",
      "trial: 0/1,  epoch: 4/5, iter: 9400,\n",
      " errors: ['0.13014', '0.13029', '0.14005']\n",
      "trial: 0/1,  epoch: 4/5, iter: 9500,\n",
      " errors: ['0.13253', '0.12977', '0.15738']\n",
      "trial: 0/1,  epoch: 4/5, iter: 9600,\n",
      " errors: ['0.11706', '0.15137', '0.12906']\n",
      "trial: 0/1,  epoch: 4/5, iter: 9700,\n",
      " errors: ['0.11885', '0.14899', '0.14587']\n",
      "trial: 0/1,  epoch: 4/5, iter: 9800,\n",
      " errors: ['0.10995', '0.16995', '0.13283']\n",
      "trial: 0/1,  epoch: 4/5, iter: 9900,\n",
      " errors: ['0.12059', '0.17773', '0.14206']\n",
      "trial: 0/1,  epoch: 4/5, iter: 10000,\n",
      " errors: ['0.12925', '0.16523', '0.15221']\n",
      "trial: 0/1,  epoch: 4/5, iter: 10100,\n",
      " errors: ['0.13795', '0.15075', '0.16676']\n",
      "trial: 0/1,  epoch: 4/5, iter: 10200,\n",
      " errors: ['0.12478', '0.17012', '0.13581']\n"
     ]
    }
   ],
   "source": [
    "# a list that records trial results in the following form (lowest_errors, best_net_params)\n",
    "trial_results = []\n",
    "best_trial_min_sum = min_sum = float('inf')\n",
    "best_trial_params = None\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    # initialize nets\n",
    "    if which_model == 1:\n",
    "        netD = Discriminator1().to(device)\n",
    "        netG = Generator1().to(device)\n",
    "    if which_model == 2:\n",
    "        netD = Discriminator2().to(device)\n",
    "        netG = Generator2().to(device)\n",
    "    elif which_model == 3:\n",
    "        netD = Discriminator3().to(device)\n",
    "        netG = Generator3().to(device)\n",
    "\n",
    "    netD.apply(weights_init)\n",
    "    netG.apply(weights_init)\n",
    "\n",
    "\n",
    "    # Initialise optimiser\n",
    "\n",
    "    # if which_optimizer == 'Adam':\n",
    "    #     optG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.99))\n",
    "    #     optD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.99))\n",
    "    # elif which_optimizer == 'RMSProp':\n",
    "    #     optG = torch.optim.RMSprop(netG.parameters(), lr=lr)\n",
    "    #     optD = torch.optim.RMSprop(netD.parameters(), lr=lr)\n",
    "\n",
    "    optG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.99))\n",
    "    optD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.99))\n",
    "\n",
    "    # Prepare early stopping\n",
    "\n",
    "    min_sum = float('inf')\n",
    "    min_sum_errors = [1.0 for i in range(a_dim)]\n",
    "    min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "    min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "\n",
    "    # Start classical training\n",
    "\n",
    "    iters = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for i, data in enumerate(dataloader):\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # weight clipping so critic is lipschitz\n",
    "            for p in netD.parameters():\n",
    "                p.data.clamp_(-weight_cliping_limit, weight_cliping_limit)\n",
    "\n",
    "            # check actual batch size (last batch could be shorter)\n",
    "            b_size = data.size(0)\n",
    "\n",
    "            # Train Discriminator\n",
    "            # first on real data\n",
    "            out_D_real = netD(data)\n",
    "            lossDr = out_D_real.mean()\n",
    "\n",
    "            # then on fake samples\n",
    "            # data has shape (b_size, w_dim + a_dim) where w_dim are the dimensions of the driving BM and a_dim is the dim of Levy Areas\n",
    "            W = data[:,:w_dim]\n",
    "            noise = torch.randn((b_size,noise_size), dtype=torch.float, device=device)\n",
    "            gen_in = torch.cat((noise,W),1)\n",
    "            # generate fake data\n",
    "            generated_A = netG(gen_in)\n",
    "            fake_in = torch.cat((W,generated_A.detach()),1)\n",
    "\n",
    "            out_D_fake = netD(fake_in)\n",
    "            lossDf = out_D_fake.mean()\n",
    "\n",
    "            lossD = lossDf - lossDr\n",
    "            lossD.backward()\n",
    "            optD.step()\n",
    "\n",
    "            # train Generator every 5 iterations\n",
    "            if iters%10 == 0:\n",
    "                netG.zero_grad()\n",
    "\n",
    "                fake_in = torch.cat((W,generated_A),1)\n",
    "                lossG = - netD(fake_in)\n",
    "                lossG = lossG.mean()\n",
    "                lossG.backward()\n",
    "                optG.step()\n",
    "\n",
    "            if iters%100 == 0:\n",
    "                # Test Wasserstein error for fixed W\n",
    "                noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "                g_in = torch.cat((noise,W_fixed),1)\n",
    "                A_fixed_gen = netG(g_in).detach().numpy()\n",
    "                errors = [sqrt(ot.wasserstein_1d(A_fixed_true[:,i],A_fixed_gen[:,i],p=2)) for i in range(a_dim)]\n",
    "\n",
    "                # Print out partial results\n",
    "                pretty_errors = [\"{0:0.5f}\".format(i) for i in errors]\n",
    "                # pretty_chen_errors = [\"{0:0.5f}\".format(i) for i in ch_err]\n",
    "                print(f\"trial: {trial}/{num_trials},  epoch: {epoch}/{num_epochs}, iter: {iters},\\n errors: {pretty_errors}\")\n",
    "\n",
    "                # Early stopping checkpoint\n",
    "                error_sum = sum(errors)\n",
    "                if error_sum <= min_sum:\n",
    "                    min_sum = error_sum\n",
    "                    min_sum_errors = errors\n",
    "                    min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "                    min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "                    print(\"Saved parameters\")\n",
    "\n",
    "            iters += 1\n",
    "\n",
    "\n",
    "    # end of trial\n",
    "    # record trial results\n",
    "\n",
    "    result = (min_sum_errors, min_sum_paramsG, min_sum_paramsD)\n",
    "    trial_results.append(result)\n",
    "\n",
    "    if min_sum < best_trial_min_sum:\n",
    "        best_trial_min_sum = min_sum\n",
    "        best_trial_params = (min_sum_paramsG, min_sum_paramsD)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# save the parameters of the best trial\n",
    "best_paramsG, best_paramsD = best_trial_params\n",
    "torch.save(best_paramsG, f'model_saves/GAN{which_model}_{w_dim}d_{num_epochs}epochs_generator.pt')\n",
    "torch.save(best_paramsD, f'model_saves/GAN{which_model}_{w_dim}d_{num_epochs}epochs_discriminator.pt')\n",
    "file_for_results = open(f'testing_results/GAN{which_model}_{w_dim}d_{num_epochs}epochs_{which_optimizer}_{lr}lr_results.obj', 'wb+')\n",
    "pickle.dump(trial_results, file_for_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0.09119092735059643], 1: [0.050501424543068676], 2: [0.15664823000335448]}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results\n",
    "\n",
    "results_per_adim = {i: [] for i in range(a_dim)}\n",
    "\n",
    "for res in trial_results:\n",
    "    errs, paramsG, paramsD = res\n",
    "    for i, err in enumerate(errs):\n",
    "        results_per_adim[i].append(err)\n",
    "\n",
    "print(results_per_adim)\n",
    "\n",
    "results_per_adim = [results_per_adim[i] for i in range(a_dim)]\n",
    "def list_pairs(m):\n",
    "    fixed_w_list = [1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7]\n",
    "    lst =[]\n",
    "    for k in range(m):\n",
    "        for l in range(k+1,m):\n",
    "            lst.append((fixed_w_list[k],fixed_w_list[l]))\n",
    "\n",
    "    return lst\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.eventplot(results_per_adim, orientation= 'vertical')\n",
    "ax.set_xticks(range(a_dim),list_pairs(w_dim))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}