{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import ot\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import copy\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "noise_size = 32\n",
    "\n",
    "# Number of training epochs using classical training\n",
    "num_epochs = 15\n",
    "\n",
    "# Number of iterations of Chen training\n",
    "num_Chen_iters = 5000\n",
    "\n",
    "# 'Adam' of 'RMSProp'\n",
    "which_optimizer = 'RMSProp'\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.00002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "ngpu = 0\n",
    "\n",
    "# To keep the criterion Lipschitz\n",
    "weight_cliping_limit = 0.01\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "test_batch_size = 65536\n",
    "\n",
    "w_dim = 4\n",
    "\n",
    "a_dim = int(w_dim*(w_dim - 1)//2)\n",
    "\n",
    "# if 1 use GAN1, if 2 use GAN2, etc.\n",
    "which_model = 2\n",
    "\n",
    "# slope for LeakyReLU\n",
    "leakyReLU_slope = 0.2\n",
    "\n",
    "# this gives the option to rum the training process multiple times with differently initialised GANs\n",
    "num_trials = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CHEN RELATION\n",
    "# Levy-area satisfies a version of the Chen relation (see Chen_relation.pdf) and is the unique distribution which satisfies this version of the relation\n",
    "\n",
    "def chen_combine(w_a_in: torch.TensorType):\n",
    "    # the batch dimension of the inputs will be quartered\n",
    "    out_size = w_a_in.size(0) // 2\n",
    "    assert 2 * out_size == w_a_in.size(0)\n",
    "    assert w_a_in.size(1) == w_dim + a_dim\n",
    "\n",
    "    # w_0_s is from 0 to t/2 and w_s_t is from t/2 to t\n",
    "    w_0_s, w_s_t = w_a_in.chunk(2)\n",
    "    result = torch.clone(w_0_s + w_s_t)\n",
    "    result[:, :w_dim] = sqrt(0.5) * result[:, :w_dim]\n",
    "    result[:, w_dim:(w_dim + a_dim)] = 0.5 * result[:, w_dim:(w_dim + a_dim)]\n",
    "\n",
    "    idx = w_dim\n",
    "    for k in range(w_dim - 1):\n",
    "        for l in range(k + 1, w_dim):\n",
    "            correction_term = 0.25 * (w_0_s[:, k] * w_s_t[:, l] - w_0_s[:, l] * w_s_t[:, k])\n",
    "            result[:, idx] += correction_term\n",
    "            idx += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# prints the 2-Wasserstein distances (in each of the Levy-area dimensions) between the input and chen_combine(chen_combine(input))\n",
    "# The idea behind this is that Levy-area is the unique distribution which is close to chen_combine of itself\n",
    "# Indeed this is experimentally confirmed in test.ipynb\n",
    "\n",
    "def chen_error_2step(w_a_in: torch.TensorType):\n",
    "    combined_data = chen_combine(w_a_in)\n",
    "    combined_data = chen_combine(combined_data)\n",
    "    return [sqrt(ot.wasserstein_1d(combined_data[:, w_dim + i], w_a_in[:, w_dim + i], p=2)) for i in range(a_dim)]\n",
    "\n",
    "\n",
    "# create dataloader for samples\n",
    "\n",
    "def row_processer(row):\n",
    "    return np.array(row, dtype=np.float32)\n",
    "\n",
    "\n",
    "filename = f\"samples/samples_{w_dim}-dim.csv\"\n",
    "datapipe = dp.iter.FileOpener([filename], mode='b')\n",
    "datapipe = datapipe.parse_csv(delimiter=',')\n",
    "datapipe = datapipe.map(row_processer)\n",
    "dataloader = DataLoader(dataset=datapipe, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    if classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# GAN 1\n",
    "\n",
    "class Generator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + noise_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "\n",
    "# GAN 2\n",
    "\n",
    "class Generator2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + noise_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "# GAN 3\n",
    "\n",
    "class Generator3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + noise_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "# A fixed W increment for testing purposes\n",
    "W_fixed: torch.Tensor = torch.tensor([1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7])\n",
    "\n",
    "\n",
    "W_fixed = W_fixed[:w_dim].unsqueeze(1).transpose(1,0)\n",
    "W_fixed = W_fixed.expand((test_batch_size,w_dim))\n",
    "\n",
    "# Load \"true\" samples generated from this fixed W increment\n",
    "test_filename = f\"samples/fixed_samples_{w_dim}-dim.csv\"\n",
    "A_fixed_true = np.genfromtxt(test_filename,dtype=float,delimiter=',',)\n",
    "A_fixed_true = A_fixed_true[:,w_dim:(w_dim+a_dim)]\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# a list that records trial results in the following form (lowest_errors, best_net_params)\n",
    "trial_results = []\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    # initialize nets\n",
    "    if which_model == 1:\n",
    "        netD = Discriminator1().to(device)\n",
    "        netG = Generator1().to(device)\n",
    "    if which_model == 2:\n",
    "        netD = Discriminator2().to(device)\n",
    "        netG = Generator2().to(device)\n",
    "    elif which_model == 3:\n",
    "        netD = Discriminator3().to(device)\n",
    "        netG = Generator3().to(device)\n",
    "\n",
    "    netD.apply(weights_init)\n",
    "    netG.apply(weights_init)\n",
    "\n",
    "\n",
    "    # Initialise optimiser\n",
    "\n",
    "    if which_optimizer == 'Adam':\n",
    "        optG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "        optD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    elif which_optimizer == 'RMSProp':\n",
    "        optG = torch.optim.RMSprop(netG.parameters(), lr=lr)\n",
    "        optD = torch.optim.RMSprop(netD.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    # Prepare early stopping\n",
    "\n",
    "    min_sum = float('inf')\n",
    "    min_sum_errors = [1.0 for i in range(a_dim)]\n",
    "    min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "    min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "\n",
    "    # Start classical training\n",
    "\n",
    "    iters = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for i, data in enumerate(dataloader):\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # weight clipping so critic is lipschitz\n",
    "            for p in netD.parameters():\n",
    "                p.data.clamp_(-weight_cliping_limit, weight_cliping_limit)\n",
    "\n",
    "            # check actual batch size (last batch could be shorter)\n",
    "            b_size = data.size(0)\n",
    "\n",
    "            # Train Discriminator\n",
    "            # first on real data\n",
    "            out_D_real = netD(data)\n",
    "            lossDr = out_D_real.mean(0).view(1)\n",
    "            lossDr.backward(one)\n",
    "\n",
    "            # then on fake samples\n",
    "            # data has shape (b_size, w_dim + a_dim) where w_dim are the dimensions of the driving BM and a_dim is the dim of Levy Areas\n",
    "            W = data[:,:w_dim]\n",
    "            A_real = data[:,w_dim:(w_dim + a_dim)]\n",
    "            noise = torch.randn((b_size,noise_size), dtype=torch.float, device=device)\n",
    "            gen_in = torch.cat((noise,W),1)\n",
    "            # generate fake data\n",
    "            generated_A = netG(gen_in)\n",
    "            fake_in = torch.cat((W,generated_A.detach()),1)\n",
    "\n",
    "            lossDf = netD(fake_in)\n",
    "            lossDf = lossDf.mean(0).view(1)\n",
    "            lossDf.backward(mone)\n",
    "            lossD = lossDr - lossDf\n",
    "            optD.step()\n",
    "\n",
    "            # train Generator every 5 iterations\n",
    "            if iters%5 == 0:\n",
    "                netG.zero_grad()\n",
    "\n",
    "                fake_in = torch.cat((W,generated_A),1)\n",
    "                lossG = netD(fake_in)\n",
    "                lossG = lossG.mean(0).view(1)\n",
    "                lossG.backward(one)\n",
    "                optG.step()\n",
    "\n",
    "            if iters%100 == 0:\n",
    "                # Test Wasserstein error for fixed W\n",
    "                noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "                g_in = torch.cat((noise,W_fixed),1)\n",
    "                A_fixed_gen = netG(g_in).detach().numpy()\n",
    "                errors = [sqrt(ot.wasserstein_1d(A_fixed_true[:,i],A_fixed_gen[:,i],p=2)) for i in range(a_dim)]\n",
    "\n",
    "                # Print out partial results\n",
    "                pretty_errors = [\"{0:0.5f}\".format(i) for i in errors]\n",
    "                # pretty_chen_errors = [\"{0:0.5f}\".format(i) for i in ch_err]\n",
    "                print(f\"epoch: {epoch}/{num_epochs}, iter: {iters},\\n errors: {pretty_errors}\")\n",
    "\n",
    "                # Early stopping checkpoint\n",
    "                error_sum = sum(errors)\n",
    "                if error_sum <= min_sum:\n",
    "                    min_sum = error_sum\n",
    "                    min_sum_errors = errors\n",
    "                    min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "                    min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "                    print(\"Saved parameters\")\n",
    "\n",
    "            iters += 1\n",
    "\n",
    "\n",
    "    # end of trial\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}