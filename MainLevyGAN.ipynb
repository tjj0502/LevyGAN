{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import ot\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import copy\n",
    "from math import sqrt\n",
    "\n",
    "from torch.autograd import grad as torch_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To cite:\n",
    "\n",
    "J. M. C. Clark and R. J. Cameron. The maximum rate of convergence of discrete approximations for Stochastic differential equations. in Stochastic Differential Systems Filtering and Control, ed. by Grigelionis (Springer, Berlin), 1980.\n",
    "A. S. Dickinson. Optimal Approximation of the Second Iterated Integral of Brownian Motion. Stochastic Analysis and Applications, 25(5):1109{1128, 2007.\n",
    "\n",
    "F. Kastner, A. Rößler. \"An Analysis of Approximation Algorithms for Iterated Stochastic Integrals and a Julia and Matlab Simulation Toolbox\". arXiv:2201.08424\n",
    "\n",
    "Foster, J. M. Numerical Approximations for Stochastic Differential Equations. University of Oxford, 2020."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "noise_size = 61\n",
    "\n",
    "# Number of training epochs using classical training\n",
    "num_epochs = 15\n",
    "\n",
    "# Number of iterations of Chen training\n",
    "num_Chen_iters = 5000\n",
    "\n",
    "# 'Adam' of 'RMSProp'\n",
    "which_optimizer = 'Adam'\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0001\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.9\n",
    "\n",
    "ngpu = 0\n",
    "\n",
    "# To keep the criterion Lipschitz\n",
    "weight_cliping_limit = 0.01\n",
    "\n",
    "# for gradient penalty\n",
    "gp_weight = 10.0\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "test_batch_size = 65536\n",
    "\n",
    "w_dim = 3\n",
    "\n",
    "a_dim = int(w_dim*(w_dim - 1)//2)\n",
    "\n",
    "# if 1 use GAN1, if 2 use GAN2, etc.\n",
    "which_model = 1\n",
    "\n",
    "# slope for LeakyReLU\n",
    "leakyReLU_slope = 0.2\n",
    "\n",
    "# this gives the option to rum the training process multiple times with differently initialised GANs\n",
    "#num_trials = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# CHEN RELATION\n",
    "# Levy-area satisfies a version of the Chen relation (see Chen_relation.pdf) and is the unique distribution which satisfies this version of the relation\n",
    "\n",
    "def chen_combine(w_a_in: torch.TensorType):\n",
    "    # the batch dimension of the inputs will be quartered\n",
    "    out_size = w_a_in.size(0)//2\n",
    "    assert 2*out_size == w_a_in.size(0)\n",
    "    assert w_a_in.size(1) == w_dim + a_dim\n",
    "\n",
    "    # w_0_s is from 0 to t/2 and w_s_t is from t/2 to t\n",
    "    w_0_s,w_s_t = w_a_in.chunk(2)\n",
    "    result = torch.clone(w_0_s + w_s_t)\n",
    "    result[:,:w_dim] = sqrt(0.5)*result[:,:w_dim]\n",
    "    result[:,w_dim:(w_dim+a_dim)] = 0.5*result[:,w_dim:(w_dim+a_dim)]\n",
    "\n",
    "    idx = w_dim\n",
    "    for k in range(w_dim - 1):\n",
    "        for l in range(k+1,w_dim):\n",
    "            correction_term = 0.25*(w_0_s[:,k]*w_s_t[:,l] - w_0_s[:,l]*w_s_t[:,k])\n",
    "            result[:,idx] += correction_term\n",
    "            idx += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "# prints the 2-Wasserstein distances (in each of the Levy-area dimensions) between the input and chen_combine(chen_combine(input))\n",
    "# The idea behind this is that Levy-area is the unique distribution which is close to chen_combine of itself\n",
    "# Indeed this is experimentally confirmed in test.ipynb\n",
    "\n",
    "def chen_error_2step(w_a_in: torch.TensorType):\n",
    "    combined_data = chen_combine(w_a_in)\n",
    "    combined_data = chen_combine(combined_data)\n",
    "    return [sqrt(ot.wasserstein_1d(combined_data[:,w_dim+i],w_a_in[:,w_dim+i],p=2)) for i in range(a_dim)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# create dataloader for samples\n",
    "\n",
    "def row_processer(row):\n",
    "    return np.array(row, dtype= np.float32)\n",
    "\n",
    "filename = f\"samples/samples_{w_dim}-dim.csv\"\n",
    "datapipe = dp.iter.FileOpener([filename], mode='b')\n",
    "datapipe = datapipe.parse_csv(delimiter=',')\n",
    "datapipe = datapipe.map(row_processer)\n",
    "dataloader = DataLoader(dataset=datapipe, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "# Check if the dimensions match\n",
    "d = next(iter(dataloader))\n",
    "if d.size(1) != a_dim + w_dim:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!! WRONG DATA DIMENSIONS !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    if classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# GAN 1\n",
    "\n",
    "class Generator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim+noise_size,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(512,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(512,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# GAN 2\n",
    "\n",
    "class Generator2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim+noise_size,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(1024,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(256,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# GAN 3\n",
    "\n",
    "class Generator3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim+noise_size,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128,a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(512,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Generator1(\n  (main): Sequential(\n    (0): Linear(in_features=64, out_features=512, bias=True)\n    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Linear(in_features=512, out_features=512, bias=True)\n    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): Linear(in_features=512, out_features=128, bias=True)\n    (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU()\n    (9): Linear(in_features=128, out_features=3, bias=True)\n  )\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize nets\n",
    "if which_model == 1:\n",
    "    netD = Discriminator1().to(device)\n",
    "    netG = Generator1().to(device)\n",
    "elif which_model == 2:\n",
    "    netD = Discriminator2().to(device)\n",
    "    netG = Generator2().to(device)\n",
    "\n",
    "\n",
    "netD.apply(weights_init)\n",
    "netG.apply(weights_init)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Initialise optimiser\n",
    "\n",
    "if which_optimizer == 'Adam':\n",
    "    optG = torch.optim.Adam(netG.parameters(),lr = lr, betas=(beta1,0.99))\n",
    "    optD = torch.optim.Adam(netD.parameters(), lr = lr, betas=(beta1,0.99))\n",
    "elif which_optimizer == 'RMSProp':\n",
    "    optG = torch.optim.RMSprop(netG.parameters(), lr = lr)\n",
    "    optD = torch.optim.RMSprop(netD.parameters(), lr = lr)\n",
    "\n",
    "# A fixed W increment for testing purposes\n",
    "W_fixed: torch.Tensor = torch.tensor([1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7])\n",
    "\n",
    "\n",
    "W_fixed = W_fixed[:w_dim].unsqueeze(1).transpose(1,0)\n",
    "W_fixed = W_fixed.expand((test_batch_size,w_dim))\n",
    "\n",
    "# Load \"true\" samples generated from this fixed W increment\n",
    "test_filename = f\"samples/fixed_samples_{w_dim}-dim.csv\"\n",
    "A_fixed_true = np.genfromtxt(test_filename,dtype=float,delimiter=',',)\n",
    "A_fixed_true = A_fixed_true[:,w_dim:(w_dim+a_dim)]\n",
    "\n",
    "wass_errors = []\n",
    "chen_errors = []\n",
    "\n",
    "iters = 0\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def _gradient_penalty(real_data, generated_data):\n",
    "    b_size_gp = real_data.size()[0]\n",
    "\n",
    "    # Calculate interpolation\n",
    "    alpha = torch.rand(b_size_gp, 1)\n",
    "    alpha = alpha.expand_as(real_data)\n",
    "    interpolated = (alpha * real_data.data + (1 - alpha) * generated_data.data).requires_grad_(True)\n",
    "\n",
    "    if ngpu > 0:\n",
    "        interpolated = interpolated.cuda()\n",
    "\n",
    "    # Calculate probability of interpolated examples\n",
    "    prob_interpolated = netD(interpolated)\n",
    "\n",
    "    # Calculate gradients of probabilities with respect to examples\n",
    "    gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                           grad_outputs=torch.ones(prob_interpolated.size()).cuda() if ngpu > 0 else torch.ones(\n",
    "                           prob_interpolated.size()),\n",
    "                           create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # Gradients have shape (b_size, num_channels, img_width, img_height),\n",
    "    # so flatten to easily take norm per example in batch\n",
    "    gradients = gradients.view(b_size_gp, -1)\n",
    "\n",
    "    # Derivatives of the gradient close to 0 can cause problems because of\n",
    "    # the square root, so manually calculate norm and add epsilon\n",
    "    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "\n",
    "    # Return gradient penalty\n",
    "    return gp_weight * ((gradients_norm - 1) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Early stopping setup\n",
    "\n",
    "# Will have two backup points:\n",
    "# One where the sum of Wasserstein errors was minimal and one where the max was minimal\n",
    "\n",
    "min_sum = float('inf')\n",
    "min_sum_errors = [1.0 for i in range(a_dim)]\n",
    "min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "\n",
    "# min_max_err = float('inf')\n",
    "# min_max_errors = [1.0 for i in range(a_dim)]\n",
    "# min_max_paramsG = copy.deepcopy(netG.state_dict())\n",
    "# min_max_paramsD = copy.deepcopy(netD.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/15, iter: 0,\n",
      " errors: ['0.23749', '0.29929', '0.32927']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 100,\n",
      " errors: ['0.24543', '0.28956', '0.33167']\n",
      "epoch: 0/15, iter: 200,\n",
      " errors: ['0.23460', '0.27949', '0.33157']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 300,\n",
      " errors: ['0.26016', '0.28202', '0.33059']\n",
      "epoch: 0/15, iter: 400,\n",
      " errors: ['0.51934', '0.28377', '0.31615']\n",
      "epoch: 0/15, iter: 500,\n",
      " errors: ['0.64397', '0.24384', '0.32927']\n",
      "epoch: 0/15, iter: 600,\n",
      " errors: ['0.60151', '0.22219', '0.44501']\n",
      "epoch: 0/15, iter: 700,\n",
      " errors: ['0.42124', '0.22643', '0.60849']\n",
      "epoch: 0/15, iter: 800,\n",
      " errors: ['0.25262', '0.21943', '0.70511']\n",
      "epoch: 0/15, iter: 900,\n",
      " errors: ['0.23452', '0.23149', '0.72049']\n",
      "epoch: 0/15, iter: 1000,\n",
      " errors: ['0.24897', '0.26585', '0.71988']\n",
      "epoch: 0/15, iter: 1100,\n",
      " errors: ['0.31746', '0.30196', '0.67305']\n",
      "epoch: 0/15, iter: 1200,\n",
      " errors: ['0.32890', '0.32628', '0.64030']\n",
      "epoch: 0/15, iter: 1300,\n",
      " errors: ['0.32109', '0.33921', '0.60617']\n",
      "epoch: 0/15, iter: 1400,\n",
      " errors: ['0.30927', '0.31330', '0.59815']\n",
      "epoch: 0/15, iter: 1500,\n",
      " errors: ['0.28541', '0.22875', '0.65109']\n",
      "epoch: 0/15, iter: 1600,\n",
      " errors: ['0.26099', '0.20228', '0.67327']\n",
      "epoch: 0/15, iter: 1700,\n",
      " errors: ['0.24899', '0.22923', '0.62058']\n",
      "epoch: 0/15, iter: 1800,\n",
      " errors: ['0.24368', '0.28522', '0.53023']\n",
      "epoch: 0/15, iter: 1900,\n",
      " errors: ['0.24100', '0.30859', '0.43734']\n",
      "epoch: 0/15, iter: 2000,\n",
      " errors: ['0.25865', '0.30269', '0.36412']\n",
      "epoch: 1/15, iter: 2100,\n",
      " errors: ['0.27936', '0.29783', '0.31936']\n",
      "epoch: 1/15, iter: 2200,\n",
      " errors: ['0.28011', '0.30038', '0.30161']\n",
      "epoch: 1/15, iter: 2300,\n",
      " errors: ['0.28094', '0.29683', '0.30899']\n",
      "epoch: 1/15, iter: 2400,\n",
      " errors: ['0.28878', '0.28218', '0.32105']\n",
      "epoch: 1/15, iter: 2500,\n",
      " errors: ['0.29341', '0.26000', '0.32849']\n",
      "epoch: 1/15, iter: 2600,\n",
      " errors: ['0.28631', '0.25447', '0.33398']\n",
      "epoch: 1/15, iter: 2700,\n",
      " errors: ['0.27749', '0.25163', '0.33186']\n",
      "epoch: 1/15, iter: 2800,\n",
      " errors: ['0.27001', '0.24621', '0.32153']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 2900,\n",
      " errors: ['0.26804', '0.23563', '0.31337']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 3000,\n",
      " errors: ['0.26600', '0.23371', '0.30440']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 3100,\n",
      " errors: ['0.26235', '0.23196', '0.29671']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 3200,\n",
      " errors: ['0.25912', '0.23409', '0.28514']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 3300,\n",
      " errors: ['0.26124', '0.23645', '0.28274']\n",
      "epoch: 1/15, iter: 3400,\n",
      " errors: ['0.26239', '0.23686', '0.27934']\n",
      "epoch: 1/15, iter: 3500,\n",
      " errors: ['0.25876', '0.22888', '0.27803']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 3600,\n",
      " errors: ['0.24879', '0.22233', '0.27374']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 3700,\n",
      " errors: ['0.24268', '0.22471', '0.27384']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 3800,\n",
      " errors: ['0.23907', '0.22633', '0.27048']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 3900,\n",
      " errors: ['0.23712', '0.21974', '0.26649']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 4000,\n",
      " errors: ['0.23797', '0.21452', '0.26456']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 4100,\n",
      " errors: ['0.23762', '0.20363', '0.26330']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 4200,\n",
      " errors: ['0.23839', '0.19900', '0.26533']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 4300,\n",
      " errors: ['0.23825', '0.20051', '0.26920']\n",
      "epoch: 2/15, iter: 4400,\n",
      " errors: ['0.23410', '0.19785', '0.26812']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 4500,\n",
      " errors: ['0.22969', '0.19548', '0.26448']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 4600,\n",
      " errors: ['0.23085', '0.19650', '0.27047']\n",
      "epoch: 2/15, iter: 4700,\n",
      " errors: ['0.23274', '0.19456', '0.26406']\n",
      "epoch: 2/15, iter: 4800,\n",
      " errors: ['0.23156', '0.19312', '0.25897']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 4900,\n",
      " errors: ['0.23068', '0.19836', '0.25970']\n",
      "epoch: 2/15, iter: 5000,\n",
      " errors: ['0.22654', '0.21231', '0.26744']\n",
      "epoch: 2/15, iter: 5100,\n",
      " errors: ['0.22074', '0.22308', '0.25424']\n",
      "epoch: 2/15, iter: 5200,\n",
      " errors: ['0.22226', '0.22261', '0.24748']\n",
      "epoch: 2/15, iter: 5300,\n",
      " errors: ['0.22089', '0.21893', '0.24318']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 5400,\n",
      " errors: ['0.21956', '0.22168', '0.24144']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 5500,\n",
      " errors: ['0.22193', '0.22841', '0.23919']\n",
      "epoch: 2/15, iter: 5600,\n",
      " errors: ['0.21829', '0.23090', '0.24390']\n",
      "epoch: 2/15, iter: 5700,\n",
      " errors: ['0.21597', '0.22422', '0.24517']\n",
      "epoch: 2/15, iter: 5800,\n",
      " errors: ['0.21670', '0.21664', '0.24100']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 5900,\n",
      " errors: ['0.21874', '0.21560', '0.23808']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 6000,\n",
      " errors: ['0.21919', '0.21154', '0.23628']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 6100,\n",
      " errors: ['0.21585', '0.20448', '0.23959']\n",
      "Saved parameters\n",
      "epoch: 3/15, iter: 6200,\n",
      " errors: ['0.21777', '0.20429', '0.23655']\n",
      "Saved parameters\n",
      "epoch: 3/15, iter: 6300,\n",
      " errors: ['0.21885', '0.20504', '0.23100']\n",
      "Saved parameters\n",
      "epoch: 3/15, iter: 6400,\n",
      " errors: ['0.21931', '0.20211', '0.23507']\n",
      "epoch: 3/15, iter: 6500,\n",
      " errors: ['0.22041', '0.20584', '0.23774']\n",
      "epoch: 3/15, iter: 6600,\n",
      " errors: ['0.21869', '0.20873', '0.23352']\n",
      "epoch: 3/15, iter: 6700,\n",
      " errors: ['0.22728', '0.21164', '0.23272']\n",
      "epoch: 3/15, iter: 6800,\n",
      " errors: ['0.22694', '0.21166', '0.23684']\n",
      "epoch: 3/15, iter: 6900,\n",
      " errors: ['0.22109', '0.20823', '0.24520']\n",
      "epoch: 3/15, iter: 7000,\n",
      " errors: ['0.21675', '0.20535', '0.25515']\n",
      "epoch: 3/15, iter: 7100,\n",
      " errors: ['0.21545', '0.20874', '0.26250']\n",
      "epoch: 3/15, iter: 7200,\n",
      " errors: ['0.21966', '0.21840', '0.26681']\n",
      "epoch: 3/15, iter: 7300,\n",
      " errors: ['0.23217', '0.22924', '0.29422']\n",
      "epoch: 3/15, iter: 7400,\n",
      " errors: ['0.23535', '0.24761', '0.36267']\n",
      "epoch: 3/15, iter: 7500,\n",
      " errors: ['0.22650', '0.25114', '0.42211']\n",
      "epoch: 3/15, iter: 7600,\n",
      " errors: ['0.22976', '0.24343', '0.41707']\n",
      "epoch: 3/15, iter: 7700,\n",
      " errors: ['0.22899', '0.24004', '0.38353']\n",
      "epoch: 3/15, iter: 7800,\n",
      " errors: ['0.23857', '0.24374', '0.35143']\n",
      "epoch: 3/15, iter: 7900,\n",
      " errors: ['0.23872', '0.23917', '0.32679']\n",
      "epoch: 3/15, iter: 8000,\n",
      " errors: ['0.25250', '0.23560', '0.27960']\n",
      "epoch: 3/15, iter: 8100,\n",
      " errors: ['0.26181', '0.23052', '0.23740']\n",
      "epoch: 4/15, iter: 8200,\n",
      " errors: ['0.24854', '0.22946', '0.23286']\n",
      "epoch: 4/15, iter: 8300,\n",
      " errors: ['0.23196', '0.22949', '0.23576']\n",
      "epoch: 4/15, iter: 8400,\n",
      " errors: ['0.22359', '0.23246', '0.22078']\n",
      "epoch: 4/15, iter: 8500,\n",
      " errors: ['0.21776', '0.23366', '0.20972']\n",
      "epoch: 4/15, iter: 8600,\n",
      " errors: ['0.22180', '0.24036', '0.21157']\n",
      "epoch: 4/15, iter: 8700,\n",
      " errors: ['0.23011', '0.25175', '0.22495']\n",
      "epoch: 4/15, iter: 8800,\n",
      " errors: ['0.24140', '0.27250', '0.23531']\n",
      "epoch: 4/15, iter: 8900,\n",
      " errors: ['0.23768', '0.29707', '0.22780']\n",
      "epoch: 4/15, iter: 9000,\n",
      " errors: ['0.22123', '0.30236', '0.21932']\n",
      "epoch: 4/15, iter: 9100,\n",
      " errors: ['0.22293', '0.29982', '0.21988']\n",
      "epoch: 4/15, iter: 9200,\n",
      " errors: ['0.23035', '0.30126', '0.22963']\n",
      "epoch: 4/15, iter: 9300,\n",
      " errors: ['0.23093', '0.30338', '0.23947']\n",
      "epoch: 4/15, iter: 9400,\n",
      " errors: ['0.23255', '0.30309', '0.24516']\n",
      "epoch: 4/15, iter: 9500,\n",
      " errors: ['0.23083', '0.30292', '0.24989']\n",
      "epoch: 4/15, iter: 9600,\n",
      " errors: ['0.22578', '0.30153', '0.24283']\n",
      "epoch: 4/15, iter: 9700,\n",
      " errors: ['0.22589', '0.29027', '0.23008']\n",
      "epoch: 4/15, iter: 9800,\n",
      " errors: ['0.22947', '0.26101', '0.23270']\n",
      "epoch: 4/15, iter: 9900,\n",
      " errors: ['0.23236', '0.23555', '0.25255']\n",
      "epoch: 4/15, iter: 10000,\n",
      " errors: ['0.23373', '0.21695', '0.27295']\n",
      "epoch: 4/15, iter: 10100,\n",
      " errors: ['0.23639', '0.21670', '0.28180']\n",
      "epoch: 4/15, iter: 10200,\n",
      " errors: ['0.23769', '0.22856', '0.26953']\n",
      "epoch: 5/15, iter: 10300,\n",
      " errors: ['0.23345', '0.22448', '0.25430']\n",
      "epoch: 5/15, iter: 10400,\n",
      " errors: ['0.23080', '0.20824', '0.24033']\n",
      "epoch: 5/15, iter: 10500,\n",
      " errors: ['0.23069', '0.22475', '0.23364']\n",
      "epoch: 5/15, iter: 10600,\n",
      " errors: ['0.23295', '0.25704', '0.22354']\n",
      "epoch: 5/15, iter: 10700,\n",
      " errors: ['0.23088', '0.26730', '0.21269']\n",
      "epoch: 5/15, iter: 10800,\n",
      " errors: ['0.23220', '0.28450', '0.20845']\n",
      "epoch: 5/15, iter: 10900,\n",
      " errors: ['0.23300', '0.30802', '0.20890']\n",
      "epoch: 5/15, iter: 11000,\n",
      " errors: ['0.23141', '0.32487', '0.21003']\n",
      "epoch: 5/15, iter: 11100,\n",
      " errors: ['0.23050', '0.33124', '0.20727']\n",
      "epoch: 5/15, iter: 11200,\n",
      " errors: ['0.23575', '0.33475', '0.20661']\n",
      "epoch: 5/15, iter: 11300,\n",
      " errors: ['0.24049', '0.33389', '0.21493']\n",
      "epoch: 5/15, iter: 11400,\n",
      " errors: ['0.23553', '0.33261', '0.23309']\n",
      "epoch: 5/15, iter: 11500,\n",
      " errors: ['0.23151', '0.32166', '0.24876']\n",
      "epoch: 5/15, iter: 11600,\n",
      " errors: ['0.23490', '0.31761', '0.25523']\n",
      "epoch: 5/15, iter: 11700,\n",
      " errors: ['0.23744', '0.31145', '0.25658']\n",
      "epoch: 5/15, iter: 11800,\n",
      " errors: ['0.24412', '0.30082', '0.25053']\n",
      "epoch: 5/15, iter: 11900,\n",
      " errors: ['0.24276', '0.28208', '0.23350']\n",
      "epoch: 5/15, iter: 12000,\n",
      " errors: ['0.23931', '0.26002', '0.22244']\n",
      "epoch: 5/15, iter: 12100,\n",
      " errors: ['0.22562', '0.25115', '0.23141']\n",
      "epoch: 5/15, iter: 12200,\n",
      " errors: ['0.21323', '0.25740', '0.25748']\n",
      "epoch: 6/15, iter: 12300,\n",
      " errors: ['0.21142', '0.26735', '0.28552']\n",
      "epoch: 6/15, iter: 12400,\n",
      " errors: ['0.21124', '0.26290', '0.31099']\n",
      "epoch: 6/15, iter: 12500,\n",
      " errors: ['0.21855', '0.24004', '0.32144']\n",
      "epoch: 6/15, iter: 12600,\n",
      " errors: ['0.22260', '0.22053', '0.30397']\n",
      "epoch: 6/15, iter: 12700,\n",
      " errors: ['0.22782', '0.21664', '0.28050']\n",
      "epoch: 6/15, iter: 12800,\n",
      " errors: ['0.22270', '0.23105', '0.26758']\n",
      "epoch: 6/15, iter: 12900,\n",
      " errors: ['0.21667', '0.26148', '0.25762']\n",
      "epoch: 6/15, iter: 13000,\n",
      " errors: ['0.20994', '0.28096', '0.25524']\n",
      "epoch: 6/15, iter: 13100,\n",
      " errors: ['0.20813', '0.24746', '0.25141']\n",
      "epoch: 6/15, iter: 13200,\n",
      " errors: ['0.20471', '0.21912', '0.24622']\n",
      "epoch: 6/15, iter: 13300,\n",
      " errors: ['0.21172', '0.23437', '0.24467']\n",
      "epoch: 6/15, iter: 13400,\n",
      " errors: ['0.22073', '0.25807', '0.25203']\n",
      "epoch: 6/15, iter: 13500,\n",
      " errors: ['0.22604', '0.27803', '0.26031']\n",
      "epoch: 6/15, iter: 13600,\n",
      " errors: ['0.22782', '0.28742', '0.26479']\n",
      "epoch: 6/15, iter: 13700,\n",
      " errors: ['0.22510', '0.28147', '0.25970']\n",
      "epoch: 6/15, iter: 13800,\n",
      " errors: ['0.22529', '0.26469', '0.24581']\n",
      "epoch: 6/15, iter: 13900,\n",
      " errors: ['0.22929', '0.25374', '0.22786']\n",
      "epoch: 6/15, iter: 14000,\n",
      " errors: ['0.24326', '0.24557', '0.21411']\n",
      "epoch: 6/15, iter: 14100,\n",
      " errors: ['0.24651', '0.23431', '0.21006']\n",
      "epoch: 6/15, iter: 14200,\n",
      " errors: ['0.24443', '0.22910', '0.20897']\n",
      "epoch: 6/15, iter: 14300,\n",
      " errors: ['0.23412', '0.24157', '0.21073']\n",
      "epoch: 7/15, iter: 14400,\n",
      " errors: ['0.22638', '0.26711', '0.21337']\n",
      "epoch: 7/15, iter: 14500,\n",
      " errors: ['0.22332', '0.27948', '0.21788']\n",
      "epoch: 7/15, iter: 14600,\n",
      " errors: ['0.21879', '0.29191', '0.22222']\n",
      "epoch: 7/15, iter: 14700,\n",
      " errors: ['0.22122', '0.30924', '0.22929']\n",
      "epoch: 7/15, iter: 14800,\n",
      " errors: ['0.21966', '0.31876', '0.24762']\n",
      "epoch: 7/15, iter: 14900,\n",
      " errors: ['0.22198', '0.29778', '0.26949']\n",
      "epoch: 7/15, iter: 15000,\n",
      " errors: ['0.22866', '0.25678', '0.27740']\n",
      "epoch: 7/15, iter: 15100,\n",
      " errors: ['0.23123', '0.21537', '0.28305']\n",
      "epoch: 7/15, iter: 15200,\n",
      " errors: ['0.23587', '0.19392', '0.28290']\n",
      "epoch: 7/15, iter: 15300,\n",
      " errors: ['0.24245', '0.18442', '0.28742']\n",
      "epoch: 7/15, iter: 15400,\n",
      " errors: ['0.25096', '0.18486', '0.29073']\n",
      "epoch: 7/15, iter: 15500,\n",
      " errors: ['0.24992', '0.18882', '0.28107']\n",
      "epoch: 7/15, iter: 15600,\n",
      " errors: ['0.24666', '0.20417', '0.26797']\n",
      "epoch: 7/15, iter: 15700,\n",
      " errors: ['0.24834', '0.23288', '0.26084']\n",
      "epoch: 7/15, iter: 15800,\n",
      " errors: ['0.24261', '0.24869', '0.25798']\n",
      "epoch: 7/15, iter: 15900,\n",
      " errors: ['0.23272', '0.24586', '0.25685']\n",
      "epoch: 7/15, iter: 16000,\n",
      " errors: ['0.23020', '0.23231', '0.25407']\n",
      "epoch: 7/15, iter: 16100,\n",
      " errors: ['0.23018', '0.21625', '0.25068']\n",
      "epoch: 7/15, iter: 16200,\n",
      " errors: ['0.22811', '0.20221', '0.25097']\n",
      "epoch: 7/15, iter: 16300,\n",
      " errors: ['0.22648', '0.19982', '0.25199']\n",
      "epoch: 8/15, iter: 16400,\n",
      " errors: ['0.22685', '0.20764', '0.25233']\n",
      "epoch: 8/15, iter: 16500,\n",
      " errors: ['0.24267', '0.21167', '0.24743']\n",
      "epoch: 8/15, iter: 16600,\n",
      " errors: ['0.27077', '0.21405', '0.24690']\n",
      "epoch: 8/15, iter: 16700,\n",
      " errors: ['0.27350', '0.21985', '0.25114']\n",
      "epoch: 8/15, iter: 16800,\n",
      " errors: ['0.24752', '0.22435', '0.26749']\n",
      "epoch: 8/15, iter: 16900,\n",
      " errors: ['0.24331', '0.23241', '0.29391']\n",
      "epoch: 8/15, iter: 17000,\n",
      " errors: ['0.25271', '0.23691', '0.32141']\n",
      "epoch: 8/15, iter: 17100,\n",
      " errors: ['0.25552', '0.24228', '0.33948']\n",
      "epoch: 8/15, iter: 17200,\n",
      " errors: ['0.24111', '0.24244', '0.35602']\n",
      "epoch: 8/15, iter: 17300,\n",
      " errors: ['0.24295', '0.24356', '0.31701']\n",
      "epoch: 8/15, iter: 17400,\n",
      " errors: ['0.28370', '0.24111', '0.28919']\n",
      "epoch: 8/15, iter: 17500,\n",
      " errors: ['0.30079', '0.24039', '0.27788']\n",
      "epoch: 8/15, iter: 17600,\n",
      " errors: ['0.26899', '0.24274', '0.27594']\n",
      "epoch: 8/15, iter: 17700,\n",
      " errors: ['0.24418', '0.23854', '0.27318']\n",
      "epoch: 8/15, iter: 17800,\n",
      " errors: ['0.24195', '0.23261', '0.27052']\n",
      "epoch: 8/15, iter: 17900,\n",
      " errors: ['0.24576', '0.23384', '0.26680']\n",
      "epoch: 8/15, iter: 18000,\n",
      " errors: ['0.24672', '0.23799', '0.26206']\n",
      "epoch: 8/15, iter: 18100,\n",
      " errors: ['0.23257', '0.23261', '0.26210']\n",
      "epoch: 8/15, iter: 18200,\n",
      " errors: ['0.22254', '0.23328', '0.26195']\n",
      "epoch: 8/15, iter: 18300,\n",
      " errors: ['0.22432', '0.23808', '0.25420']\n",
      "epoch: 8/15, iter: 18400,\n",
      " errors: ['0.23424', '0.24164', '0.24774']\n",
      "epoch: 9/15, iter: 18500,\n",
      " errors: ['0.24083', '0.24408', '0.24546']\n",
      "epoch: 9/15, iter: 18600,\n",
      " errors: ['0.24399', '0.24212', '0.24247']\n",
      "epoch: 9/15, iter: 18700,\n",
      " errors: ['0.24744', '0.24286', '0.24502']\n",
      "epoch: 9/15, iter: 18800,\n",
      " errors: ['0.24278', '0.25930', '0.25015']\n",
      "epoch: 9/15, iter: 18900,\n",
      " errors: ['0.23063', '0.28705', '0.25393']\n",
      "epoch: 9/15, iter: 19000,\n",
      " errors: ['0.23172', '0.31739', '0.26375']\n",
      "epoch: 9/15, iter: 19100,\n",
      " errors: ['0.23807', '0.34179', '0.28700']\n",
      "epoch: 9/15, iter: 19200,\n",
      " errors: ['0.23310', '0.33469', '0.33333']\n",
      "epoch: 9/15, iter: 19300,\n",
      " errors: ['0.22730', '0.28405', '0.34152']\n",
      "epoch: 9/15, iter: 19400,\n",
      " errors: ['0.22644', '0.24857', '0.31181']\n",
      "epoch: 9/15, iter: 19500,\n",
      " errors: ['0.22475', '0.23098', '0.29917']\n",
      "epoch: 9/15, iter: 19600,\n",
      " errors: ['0.22087', '0.20497', '0.30844']\n",
      "epoch: 9/15, iter: 19700,\n",
      " errors: ['0.22321', '0.20929', '0.32725']\n",
      "epoch: 9/15, iter: 19800,\n",
      " errors: ['0.24096', '0.25723', '0.35119']\n",
      "epoch: 9/15, iter: 19900,\n",
      " errors: ['0.27328', '0.32304', '0.37819']\n",
      "epoch: 9/15, iter: 20000,\n",
      " errors: ['0.35201', '0.41208', '0.40176']\n",
      "epoch: 9/15, iter: 20100,\n",
      " errors: ['0.43772', '0.47615', '0.42896']\n",
      "epoch: 9/15, iter: 20200,\n",
      " errors: ['0.47683', '0.41620', '0.43958']\n",
      "epoch: 9/15, iter: 20300,\n",
      " errors: ['0.45931', '0.27774', '0.44923']\n",
      "epoch: 9/15, iter: 20400,\n",
      " errors: ['0.39530', '0.21940', '0.46934']\n",
      "epoch: 10/15, iter: 20500,\n",
      " errors: ['0.30054', '0.23095', '0.45802']\n",
      "epoch: 10/15, iter: 20600,\n",
      " errors: ['0.23276', '0.24849', '0.40883']\n",
      "epoch: 10/15, iter: 20700,\n",
      " errors: ['0.21866', '0.25161', '0.35553']\n",
      "epoch: 10/15, iter: 20800,\n",
      " errors: ['0.22049', '0.24254', '0.34939']\n",
      "epoch: 10/15, iter: 20900,\n",
      " errors: ['0.21979', '0.23950', '0.36008']\n",
      "epoch: 10/15, iter: 21000,\n",
      " errors: ['0.21955', '0.23754', '0.34254']\n",
      "epoch: 10/15, iter: 21100,\n",
      " errors: ['0.22304', '0.23419', '0.32160']\n",
      "epoch: 10/15, iter: 21200,\n",
      " errors: ['0.22939', '0.23297', '0.31434']\n",
      "epoch: 10/15, iter: 21300,\n",
      " errors: ['0.23336', '0.23477', '0.30625']\n",
      "epoch: 10/15, iter: 21400,\n",
      " errors: ['0.23642', '0.24614', '0.30294']\n",
      "epoch: 10/15, iter: 21500,\n",
      " errors: ['0.24196', '0.25335', '0.30271']\n",
      "epoch: 10/15, iter: 21600,\n",
      " errors: ['0.26276', '0.24512', '0.30314']\n",
      "epoch: 10/15, iter: 21700,\n",
      " errors: ['0.27179', '0.23963', '0.30695']\n",
      "epoch: 10/15, iter: 21800,\n",
      " errors: ['0.26255', '0.23919', '0.30489']\n",
      "epoch: 10/15, iter: 21900,\n",
      " errors: ['0.25524', '0.23747', '0.29238']\n",
      "epoch: 10/15, iter: 22000,\n",
      " errors: ['0.24668', '0.24478', '0.28566']\n",
      "epoch: 10/15, iter: 22100,\n",
      " errors: ['0.23651', '0.25170', '0.28677']\n",
      "epoch: 10/15, iter: 22200,\n",
      " errors: ['0.23188', '0.25289', '0.28542']\n",
      "epoch: 10/15, iter: 22300,\n",
      " errors: ['0.23037', '0.24842', '0.27910']\n",
      "epoch: 10/15, iter: 22400,\n",
      " errors: ['0.22928', '0.24535', '0.27562']\n",
      "epoch: 10/15, iter: 22500,\n",
      " errors: ['0.22910', '0.23906', '0.26858']\n",
      "epoch: 11/15, iter: 22600,\n",
      " errors: ['0.23072', '0.23663', '0.27047']\n",
      "epoch: 11/15, iter: 22700,\n",
      " errors: ['0.22937', '0.23550', '0.27149']\n",
      "epoch: 11/15, iter: 22800,\n",
      " errors: ['0.22944', '0.23411', '0.27062']\n",
      "epoch: 11/15, iter: 22900,\n",
      " errors: ['0.23172', '0.23522', '0.27322']\n",
      "epoch: 11/15, iter: 23000,\n",
      " errors: ['0.23216', '0.23402', '0.27134']\n",
      "epoch: 11/15, iter: 23100,\n",
      " errors: ['0.23735', '0.23194', '0.27156']\n",
      "epoch: 11/15, iter: 23200,\n",
      " errors: ['0.24671', '0.23949', '0.27468']\n",
      "epoch: 11/15, iter: 23300,\n",
      " errors: ['0.25856', '0.28049', '0.28938']\n",
      "epoch: 11/15, iter: 23400,\n",
      " errors: ['0.26820', '0.32081', '0.31094']\n",
      "epoch: 11/15, iter: 23500,\n",
      " errors: ['0.27074', '0.33080', '0.32110']\n",
      "epoch: 11/15, iter: 23600,\n",
      " errors: ['0.26644', '0.29984', '0.31412']\n",
      "epoch: 11/15, iter: 23700,\n",
      " errors: ['0.24522', '0.25332', '0.29615']\n",
      "epoch: 11/15, iter: 23800,\n",
      " errors: ['0.22960', '0.23459', '0.28769']\n",
      "epoch: 11/15, iter: 23900,\n",
      " errors: ['0.22842', '0.23289', '0.28587']\n",
      "epoch: 11/15, iter: 24000,\n",
      " errors: ['0.22672', '0.23185', '0.29097']\n",
      "epoch: 11/15, iter: 24100,\n",
      " errors: ['0.22573', '0.23572', '0.29614']\n",
      "epoch: 11/15, iter: 24200,\n",
      " errors: ['0.22783', '0.24616', '0.30201']\n",
      "epoch: 11/15, iter: 24300,\n",
      " errors: ['0.22935', '0.25479', '0.30203']\n",
      "epoch: 11/15, iter: 24400,\n",
      " errors: ['0.23245', '0.25831', '0.30003']\n",
      "epoch: 11/15, iter: 24500,\n",
      " errors: ['0.23380', '0.25409', '0.29351']\n",
      "epoch: 12/15, iter: 24600,\n",
      " errors: ['0.23626', '0.25158', '0.28632']\n",
      "epoch: 12/15, iter: 24700,\n",
      " errors: ['0.23475', '0.24694', '0.28432']\n",
      "epoch: 12/15, iter: 24800,\n",
      " errors: ['0.23447', '0.24524', '0.28756']\n",
      "epoch: 12/15, iter: 24900,\n",
      " errors: ['0.23424', '0.24388', '0.29988']\n",
      "epoch: 12/15, iter: 25000,\n",
      " errors: ['0.23911', '0.24124', '0.31544']\n",
      "epoch: 12/15, iter: 25100,\n",
      " errors: ['0.23610', '0.23932', '0.31217']\n",
      "epoch: 12/15, iter: 25200,\n",
      " errors: ['0.23072', '0.24229', '0.30056']\n",
      "epoch: 12/15, iter: 25300,\n",
      " errors: ['0.23892', '0.25646', '0.30024']\n",
      "epoch: 12/15, iter: 25400,\n",
      " errors: ['0.24119', '0.26242', '0.28880']\n",
      "epoch: 12/15, iter: 25500,\n",
      " errors: ['0.22779', '0.24859', '0.26573']\n",
      "epoch: 12/15, iter: 25600,\n",
      " errors: ['0.22471', '0.24034', '0.26991']\n",
      "epoch: 12/15, iter: 25700,\n",
      " errors: ['0.22287', '0.23834', '0.29699']\n",
      "epoch: 12/15, iter: 25800,\n",
      " errors: ['0.22851', '0.26029', '0.35794']\n",
      "epoch: 12/15, iter: 25900,\n",
      " errors: ['0.24624', '0.29851', '0.42350']\n",
      "epoch: 12/15, iter: 26000,\n",
      " errors: ['0.27197', '0.28999', '0.40835']\n",
      "epoch: 12/15, iter: 26100,\n",
      " errors: ['0.28675', '0.26615', '0.35970']\n",
      "epoch: 12/15, iter: 26200,\n",
      " errors: ['0.28426', '0.24308', '0.30989']\n",
      "epoch: 12/15, iter: 26300,\n",
      " errors: ['0.26053', '0.23599', '0.28727']\n",
      "epoch: 12/15, iter: 26400,\n",
      " errors: ['0.25079', '0.25115', '0.29054']\n",
      "epoch: 12/15, iter: 26500,\n",
      " errors: ['0.25786', '0.24914', '0.29122']\n",
      "epoch: 12/15, iter: 26600,\n",
      " errors: ['0.26436', '0.25042', '0.28750']\n",
      "epoch: 13/15, iter: 26700,\n",
      " errors: ['0.26794', '0.25761', '0.28469']\n",
      "epoch: 13/15, iter: 26800,\n",
      " errors: ['0.27282', '0.29557', '0.29211']\n",
      "epoch: 13/15, iter: 26900,\n",
      " errors: ['0.27983', '0.35848', '0.33323']\n",
      "epoch: 13/15, iter: 27000,\n",
      " errors: ['0.29411', '0.33896', '0.38712']\n",
      "epoch: 13/15, iter: 27100,\n",
      " errors: ['0.29834', '0.26659', '0.39337']\n",
      "epoch: 13/15, iter: 27200,\n",
      " errors: ['0.28435', '0.25593', '0.36666']\n",
      "epoch: 13/15, iter: 27300,\n",
      " errors: ['0.26740', '0.27494', '0.33394']\n",
      "epoch: 13/15, iter: 27400,\n",
      " errors: ['0.26131', '0.27952', '0.32101']\n",
      "epoch: 13/15, iter: 27500,\n",
      " errors: ['0.26660', '0.26660', '0.32602']\n",
      "epoch: 13/15, iter: 27600,\n",
      " errors: ['0.27461', '0.24808', '0.32512']\n",
      "epoch: 13/15, iter: 27700,\n",
      " errors: ['0.27596', '0.23635', '0.31275']\n",
      "epoch: 13/15, iter: 27800,\n",
      " errors: ['0.27097', '0.23708', '0.30017']\n",
      "epoch: 13/15, iter: 27900,\n",
      " errors: ['0.26129', '0.23767', '0.29729']\n",
      "epoch: 13/15, iter: 28000,\n",
      " errors: ['0.25728', '0.23879', '0.30009']\n",
      "epoch: 13/15, iter: 28100,\n",
      " errors: ['0.25473', '0.24347', '0.29976']\n",
      "epoch: 13/15, iter: 28200,\n",
      " errors: ['0.25141', '0.25468', '0.29514']\n",
      "epoch: 13/15, iter: 28300,\n",
      " errors: ['0.25172', '0.26561', '0.28919']\n",
      "epoch: 13/15, iter: 28400,\n",
      " errors: ['0.25075', '0.27291', '0.28599']\n",
      "epoch: 13/15, iter: 28500,\n",
      " errors: ['0.24979', '0.27746', '0.28270']\n",
      "epoch: 13/15, iter: 28600,\n",
      " errors: ['0.24918', '0.27622', '0.27794']\n",
      "epoch: 14/15, iter: 28700,\n",
      " errors: ['0.24845', '0.28505', '0.28190']\n",
      "epoch: 14/15, iter: 28800,\n",
      " errors: ['0.25012', '0.29700', '0.28610']\n",
      "epoch: 14/15, iter: 28900,\n",
      " errors: ['0.26028', '0.31025', '0.29024']\n",
      "epoch: 14/15, iter: 29000,\n",
      " errors: ['0.25670', '0.29669', '0.28120']\n",
      "epoch: 14/15, iter: 29100,\n",
      " errors: ['0.25379', '0.29093', '0.27614']\n",
      "epoch: 14/15, iter: 29200,\n",
      " errors: ['0.25343', '0.28825', '0.27314']\n",
      "epoch: 14/15, iter: 29300,\n",
      " errors: ['0.26021', '0.29606', '0.27308']\n",
      "epoch: 14/15, iter: 29400,\n",
      " errors: ['0.26726', '0.30282', '0.27273']\n",
      "epoch: 14/15, iter: 29500,\n",
      " errors: ['0.26383', '0.29540', '0.26698']\n",
      "epoch: 14/15, iter: 29600,\n",
      " errors: ['0.26108', '0.28507', '0.26088']\n",
      "epoch: 14/15, iter: 29700,\n",
      " errors: ['0.24616', '0.26553', '0.25350']\n",
      "epoch: 14/15, iter: 29800,\n",
      " errors: ['0.24108', '0.25304', '0.24967']\n",
      "epoch: 14/15, iter: 29900,\n",
      " errors: ['0.24693', '0.24380', '0.24613']\n",
      "epoch: 14/15, iter: 30000,\n",
      " errors: ['0.24580', '0.23565', '0.24431']\n",
      "epoch: 14/15, iter: 30100,\n",
      " errors: ['0.23442', '0.22877', '0.24020']\n",
      "epoch: 14/15, iter: 30200,\n",
      " errors: ['0.22985', '0.23219', '0.24157']\n",
      "epoch: 14/15, iter: 30300,\n",
      " errors: ['0.22867', '0.28043', '0.25130']\n",
      "epoch: 14/15, iter: 30400,\n",
      " errors: ['0.23427', '0.43500', '0.30930']\n",
      "epoch: 14/15, iter: 30500,\n",
      " errors: ['0.27151', '0.56255', '0.37932']\n",
      "epoch: 14/15, iter: 30600,\n",
      " errors: ['0.33953', '0.54017', '0.40290']\n",
      "epoch: 14/15, iter: 30700,\n",
      " errors: ['0.42187', '0.45447', '0.39558']\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # weight clipping so critic is lipschitz\n",
    "        # for p in netD.parameters():\n",
    "        #     p.data.clamp_(-weight_cliping_limit, weight_cliping_limit)\n",
    "\n",
    "        # check actual batch size (last batch could be shorter)\n",
    "        b_size = data.size(0)\n",
    "\n",
    "        noise = torch.randn((b_size,noise_size), dtype=torch.float, device=device)\n",
    "        w = data[:,:w_dim]\n",
    "        z = torch.cat([noise,w], dim=1)\n",
    "        generated_A = netG(z)\n",
    "        fake_data = torch.cat([w,generated_A.detach()], dim=1)\n",
    "        fake_data = fake_data.detach()\n",
    "\n",
    "        gradient_penalty = _gradient_penalty(data,fake_data)\n",
    "\n",
    "        prob_real = netD(data)\n",
    "\n",
    "        prob_fake = netD(fake_data)\n",
    "\n",
    "        lossD_fake = prob_fake.mean()\n",
    "        lossD_real = prob_real.mean()\n",
    "        lossD = lossD_fake - lossD_real + gradient_penalty\n",
    "        lossD.backward()\n",
    "        optD.step()\n",
    "\n",
    "        # train Generator with probability 1/5\n",
    "        if iters%10 == 0:\n",
    "            netG.zero_grad()\n",
    "\n",
    "            fake_in = torch.cat([w,generated_A],dim=1)\n",
    "            lossG = netD(fake_in)\n",
    "            lossG = - lossG.mean()\n",
    "            lossG.backward()\n",
    "            optG.step()\n",
    "\n",
    "        if iters%100 == 0:\n",
    "            # Test Wasserstein error for fixed W\n",
    "            noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "            g_in = torch.cat((noise,W_fixed),1)\n",
    "            A_fixed_gen = netG(g_in).detach().numpy()\n",
    "            errors = [sqrt(ot.wasserstein_1d(A_fixed_true[:,i],A_fixed_gen[:,i],p=2)) for i in range(a_dim)]\n",
    "\n",
    "            # Test Chen discrepancy\n",
    "            # W = torch.randn((4*batch_size, w_dim), dtype= torch.float, device=device)\n",
    "            # noise = torch.randn((4*batch_size,noise_size), dtype=torch.float, device=device)\n",
    "            # gen_in = torch.cat((noise,W),1)\n",
    "            # A_gen = netG(gen_in)\n",
    "            # w_a = torch.cat((W,A_gen.detach()),1)\n",
    "            # ch_err = chen_error_2step(w_a)\n",
    "\n",
    "            # Print out partial results\n",
    "            pretty_errors = [\"{0:0.5f}\".format(i) for i in errors]\n",
    "            # pretty_chen_errors = [\"{0:0.5f}\".format(i) for i in ch_err]\n",
    "            print(f\"epoch: {epoch}/{num_epochs}, iter: {iters},\\n errors: {pretty_errors}\")\n",
    "            # Save for plotting\n",
    "            wass_errors.append(errors)\n",
    "            # chen_errors.append(ch_err)\n",
    "\n",
    "            # Early stopping checkpoint\n",
    "            error_sum = sum(errors)\n",
    "            if error_sum <= min_sum:\n",
    "                min_sum = error_sum\n",
    "                min_sum_errors = errors\n",
    "                min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "                min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "                print(\"Saved parameters\")\n",
    "\n",
    "        iters += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "W_fixed = [1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7]\n",
    "list_pairs(5) = [(1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)]\n",
    "\n",
    "GAN2 best: ['0.06348', '0.25953', '0.06187', '0.11594', '0.12005', '0.11992', '0.07918', '0.15956', '0.16242', '0.01383']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Return to early stopping checkpoint\n",
    "if which_model == 1:\n",
    "    best_netG = Generator1().to(device)\n",
    "elif which_model == 2:\n",
    "    best_netG = Generator2().to(device)\n",
    "\n",
    "best_netG.load_state_dict(min_sum_paramsG)\n",
    "\n",
    "torch.save(min_sum_paramsG, f'model_saves/GAN2_{w_dim}d_24epochs_generator.pt')\n",
    "torch.save(min_sum_paramsD, f'model_saves/GAN2_{w_dim}d_24epochs_discriminator.pt')\n",
    "# best_netD = Discriminator()\n",
    "# best_netD.load_state_dict(min_sum_paramsD)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test Wasserstein error for fixed W\n",
    "noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "g_in = torch.cat((noise,W_fixed),1)\n",
    "A_fixed_gen = best_netG(g_in).detach().numpy()\n",
    "errors = [sqrt(ot.wasserstein_1d(A_fixed_true[:,i],A_fixed_gen[:,i],p=2)) for i in range(a_dim)]\n",
    "\n",
    "# Print out partial results\n",
    "pretty_errors = [\"{0:0.5f}\".format(i) for i in errors]\n",
    "print(f\"best net errors: {pretty_errors}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "best net errors: ['0.06348', '0.25953', '0.06187', '0.11594', '0.12005', '0.11992', '0.07918', '0.15956', '0.16242', '0.01383']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw errors through iterations\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"2-Wasserstein distance of generated samples from the original samples for fixed W increment\")\n",
    "plt.plot(wass_errors)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Wasserstein distance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"2-Wasserstein distances after 2-step Chen recombinations\")\n",
    "plt.plot(chen_errors)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"2-Wasserstein distance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chen_iters = 0\n",
    "chen_training_wass_errors = []\n",
    "chen_training_chen_errors = []\n",
    "for i in range(num_Chen_iters):\n",
    "    netD.zero_grad()\n",
    "\n",
    "    # weight clipping so critic is lipschitz\n",
    "    for p in netD.parameters():\n",
    "        p.data.clamp_(-weight_cliping_limit, weight_cliping_limit)\n",
    "\n",
    "    # Train Discriminator\n",
    "    # generate 4*batch_size of fake data\n",
    "    W = torch.randn((4*batch_size, w_dim), dtype= torch.float, device=device)\n",
    "    noise = torch.randn((4*batch_size,noise_size), dtype=torch.float, device=device)\n",
    "    gen_in = torch.cat((noise,W),1)\n",
    "    A_gen = netG(gen_in)\n",
    "    fake_in = torch.cat((W,A_gen.detach()),1)\n",
    "    lossD_fake = netD(fake_in)\n",
    "    lossD_fake = lossD_fake.mean(0).view(1)\n",
    "    lossD_fake.backward(mone)\n",
    "\n",
    "    # now use chen_combine to produce \"true\" data from the fake one\n",
    "    # using chen_combine twice reduces batch dimension from 4*batch_size to batch_size\n",
    "    true_data = chen_combine(fake_in.detach())\n",
    "    true_data = chen_combine(true_data)\n",
    "    true_data = chen_combine(true_data)\n",
    "    assert true_data.size(0) == batch_size//2\n",
    "\n",
    "    lossD_real = netD(true_data)\n",
    "    lossD_real = 8 * lossD_real.mean(0).view(1) # multiply by 4 to counteract the 4x smaller batch\n",
    "    lossD_real.backward(one)\n",
    "    optD.step()\n",
    "\n",
    "    # train Generator with probability 1/5\n",
    "    # if np.random.randint(1,6) == 5:\n",
    "    if True:\n",
    "        netG.zero_grad()\n",
    "\n",
    "        fake_in = torch.cat((W,A_gen),1)\n",
    "        lossG = netD(fake_in)\n",
    "        lossG = lossG.mean(0).view(1)\n",
    "        lossG.backward(one)\n",
    "        optG.step()\n",
    "\n",
    "    if chen_iters%100 == 0:\n",
    "        # Test Wasserstein error for fixed W\n",
    "        noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "        g_in = torch.cat((noise,W_fixed),1)\n",
    "        A_fixed_gen = netG(g_in).detach().numpy()\n",
    "        errors = [sqrt(ot.wasserstein_1d(A_fixed_true[:,i],A_fixed_gen[:,i],p=2)) for i in range(a_dim)]\n",
    "        chen_training_wass_errors.append(errors)\n",
    "\n",
    "        # Test Chen discrepancy\n",
    "        W = torch.randn((4*batch_size, w_dim), dtype= torch.float, device=device)\n",
    "        noise = torch.randn((4*batch_size,noise_size), dtype=torch.float, device=device)\n",
    "        gen_in = torch.cat((noise,W),1)\n",
    "        A_gen = netG(gen_in)\n",
    "        w_a = torch.cat((W,A_gen.detach()),1)\n",
    "        ch_err = chen_error_2step(w_a)\n",
    "\n",
    "        # Print out partial results\n",
    "        pretty_errors = [\"{0:0.5f}\".format(i) for i in errors]\n",
    "        pretty_chen_errors = [\"{0:0.5f}\".format(i) for i in ch_err]\n",
    "        print(f\"iter: {chen_iters}/{num_Chen_iters},\\n errors: {pretty_errors}, \\n chen errors: {pretty_chen_errors}\")\n",
    "        # Save for plotting\n",
    "        chen_training_wass_errors.append(errors)\n",
    "        chen_training_chen_errors.append(ch_err)\n",
    "\n",
    "\n",
    "        # Early stopping checkpoint\n",
    "        error_sum = sum(errors)\n",
    "        if error_sum <= min_sum:\n",
    "            min_sum = error_sum\n",
    "            min_sum_errors = errors\n",
    "            min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "            min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "            print(\"Saved parameters\")\n",
    "\n",
    "    chen_iters += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw errors through iterations\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"2-Wasserstein distance of generated samples from the original samples for fixed W increment\")\n",
    "plt.plot(chen_training_wass_errors)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Wasserstein distance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Time measurements\n",
    "\n",
    "W_fixed: torch.Tensor = torch.tensor([1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7])\n",
    "W_fixed = W_fixed[:w_dim].unsqueeze(1).transpose(1,0)\n",
    "W_fixed = W_fixed.expand((test_batch_size,w_dim))\n",
    "noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "g_in = torch.cat((noise,W_fixed),1)\n",
    "netG.eval()\n",
    "start_time = timeit.default_timer()\n",
    "for i in range(100):\n",
    "    A_fixed_out=netG(g_in)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Takes 34.2s to generate 6553600 samples (original GAN)\n",
    "Calling iterated_integrals(h = 1.0, err = 0.0005) 6553600-times takes 100.5s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}