{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import ot\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import copy\n",
    "from math import sqrt\n",
    "\n",
    "from torch.autograd import grad as torch_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To cite:\n",
    "\n",
    "J. M. C. Clark and R. J. Cameron. The maximum rate of convergence of discrete approximations for Stochastic differential equations. in Stochastic Differential Systems Filtering and Control, ed. by Grigelionis (Springer, Berlin), 1980.\n",
    "A. S. Dickinson. Optimal Approximation of the Second Iterated Integral of Brownian Motion. Stochastic Analysis and Applications, 25(5):1109{1128, 2007.\n",
    "\n",
    "F. Kastner, A. Rößler. \"An Analysis of Approximation Algorithms for Iterated Stochastic Integrals and a Julia and Matlab Simulation Toolbox\". arXiv:2201.08424\n",
    "\n",
    "Foster, J. M. Numerical Approximations for Stochastic Differential Equations. University of Oxford, 2020."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "noise_size = 62\n",
    "\n",
    "# Number of training epochs using classical training\n",
    "num_epochs = 10\n",
    "\n",
    "# Number of iterations of Chen training\n",
    "num_Chen_iters = 5000\n",
    "\n",
    "# 'Adam' of 'RMSProp'\n",
    "which_optimizer = 'Adam'\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0001\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.9\n",
    "\n",
    "ngpu = 0\n",
    "\n",
    "# To keep the criterion Lipschitz\n",
    "weight_cliping_limit = 0.01\n",
    "\n",
    "# for gradient penalty\n",
    "gp_weight = 10.0\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "test_batch_size = 65536\n",
    "\n",
    "w_dim = 2\n",
    "\n",
    "a_dim = int(w_dim*(w_dim - 1)//2)\n",
    "\n",
    "# if 1 use GAN1, if 2 use GAN2, etc.\n",
    "which_model = 2\n",
    "\n",
    "# slope for LeakyReLU\n",
    "leakyReLU_slope = 0.2\n",
    "\n",
    "# this gives the option to rum the training process multiple times with differently initialised GANs\n",
    "#num_trials = 1\n",
    "\n",
    "num_tests_for2d = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# CHEN RELATION\n",
    "# Levy-area satisfies a version of the Chen relation (see Chen_relation.pdf) and is the unique distribution which satisfies this version of the relation\n",
    "\n",
    "def chen_combine(w_a_in: torch.TensorType):\n",
    "    # the batch dimension of the inputs will be quartered\n",
    "    out_size = w_a_in.size(0)//2\n",
    "    assert 2*out_size == w_a_in.size(0)\n",
    "    assert w_a_in.size(1) == w_dim + a_dim\n",
    "\n",
    "    # w_0_s is from 0 to t/2 and w_s_t is from t/2 to t\n",
    "    w_0_s,w_s_t = w_a_in.chunk(2)\n",
    "    result = torch.clone(w_0_s + w_s_t)\n",
    "    result[:,:w_dim] = sqrt(0.5)*result[:,:w_dim]\n",
    "    result[:,w_dim:(w_dim+a_dim)] = 0.5*result[:,w_dim:(w_dim+a_dim)]\n",
    "\n",
    "    idx = w_dim\n",
    "    for k in range(w_dim - 1):\n",
    "        for l in range(k+1,w_dim):\n",
    "            correction_term = 0.25*(w_0_s[:,k]*w_s_t[:,l] - w_0_s[:,l]*w_s_t[:,k])\n",
    "            result[:,idx] += correction_term\n",
    "            idx += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# prints the 2-Wasserstein distances (in each of the Levy-area dimensions) between the input and chen_combine(chen_combine(input))\n",
    "# The idea behind this is that Levy-area is the unique distribution which is close to chen_combine of itself\n",
    "# Indeed this is experimentally confirmed in test.ipynb\n",
    "\n",
    "def chen_error_3step(w_a_in: torch.TensorType):\n",
    "    combined_data = chen_combine(w_a_in)\n",
    "    combined_data = chen_combine(combined_data)\n",
    "    combined_data = chen_combine(combined_data)\n",
    "    return [sqrt(ot.wasserstein_1d(combined_data[:,w_dim+i],w_a_in[:,w_dim+i],p=2)) for i in range(a_dim)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# create dataloader for samples\n",
    "\n",
    "def row_processer(row):\n",
    "    return np.array(row, dtype= np.float32)\n",
    "\n",
    "filename = f\"samples/samples_{w_dim}-dim.csv\"\n",
    "datapipe = dp.iter.FileOpener([filename], mode='t')\n",
    "datapipe = datapipe.parse_csv(delimiter=',')\n",
    "datapipe = datapipe.map(row_processer)\n",
    "dataloader = DataLoader(dataset=datapipe, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "# Check if the dimensions match\n",
    "d = next(iter(dataloader))\n",
    "if d.size(1) != a_dim + w_dim:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!! WRONG DATA DIMENSIONS !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    if classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# GAN 1\n",
    "\n",
    "class Generator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim+noise_size,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(512,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(512,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# GAN 2\n",
    "\n",
    "class Generator2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim+noise_size,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # nn.Linear(1024,1024),\n",
    "            # nn.BatchNorm1d(1024),\n",
    "            # nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128,a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(128,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            # nn.Linear(1024,1024),\n",
    "            # nn.BatchNorm1d(1024),\n",
    "            # nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(1024,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# GAN 3\n",
    "\n",
    "class Generator3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator3, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim+noise_size,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128,a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator3, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(512,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(leakyReLU_slope),\n",
    "\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize nets\n",
    "if which_model == 1:\n",
    "    netD = Discriminator1().to(device)\n",
    "    netG = Generator1().to(device)\n",
    "elif which_model == 2:\n",
    "    netD = Discriminator2().to(device)\n",
    "    netG = Generator2().to(device)\n",
    "elif which_model == 3:\n",
    "    netD = Discriminator3().to(device)\n",
    "    netG = Generator3().to(device)\n",
    "\n",
    "netG.load_state_dict(torch.load(f'model_saves/GAN2_2d_randomly_excellent_generator.pt'))\n",
    "netD.load_state_dict(torch.load(f'model_saves/GAN2_2d_randomly_excellent_discriminator.pt'))\n",
    "\n",
    "# netD.apply(weights_init)\n",
    "# netG.apply(weights_init)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Initialise optimiser\n",
    "\n",
    "if which_optimizer == 'Adam':\n",
    "    optG = torch.optim.Adam(netG.parameters(),lr = lr, betas=(beta1,0.999))\n",
    "    optD = torch.optim.Adam(netD.parameters(), lr = lr, betas=(beta1,0.999))\n",
    "elif which_optimizer == 'RMSProp':\n",
    "    optG = torch.optim.RMSprop(netG.parameters(), lr = lr)\n",
    "    optD = torch.optim.RMSprop(netD.parameters(), lr = lr)\n",
    "\n",
    "# A fixed W increment for testing purposes\n",
    "W_fixed_whole: torch.Tensor = torch.tensor([1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7])\n",
    "\n",
    "W_fixed = W_fixed_whole[:w_dim].unsqueeze(1).transpose(1,0)\n",
    "W_fixed = W_fixed.expand((test_batch_size,w_dim))\n",
    "\n",
    "# Load \"true\" samples generated from this fixed W increment\n",
    "test_filename = f\"samples/fixed_samples_{w_dim}-dim.csv\"\n",
    "A_fixed_true = np.genfromtxt(test_filename,dtype=float,delimiter=',',)\n",
    "A_fixed_true = A_fixed_true[:,w_dim:(w_dim+a_dim)]\n",
    "\n",
    "wass_errors = []\n",
    "chen_errors = []\n",
    "\n",
    "iters = 0\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "\n",
    "# testing data for w_dim = 2\n",
    "fixed_data = [np.genfromtxt(f\"samples/fixed_samples_2-dim{i+1}.csv\",dtype=float,delimiter=',') for i in range(num_tests_for2d)]\n",
    "\n",
    "\n",
    "def all_2dim_errors(_netG):\n",
    "    errs = []\n",
    "    ch_errs = []\n",
    "    for i in range(num_tests_for2d):\n",
    "        # Test Wasserstein error for fixed W\n",
    "        data_fixed_true = fixed_data[i]\n",
    "        A_fixed_true = data_fixed_true[:,2]\n",
    "        W_combo = torch.tensor(data_fixed_true[:,:2], dtype= torch.float)\n",
    "        noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "        g_in = torch.cat((noise,W_combo),1)\n",
    "        A_fixed_gen = _netG(g_in).detach().numpy().squeeze()\n",
    "        errs.append(sqrt(ot.wasserstein_1d(A_fixed_true,A_fixed_gen,p=2)))\n",
    "\n",
    "    return errs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def _gradient_penalty(real_data, generated_data):\n",
    "    b_size_gp = real_data.size()[0]\n",
    "\n",
    "    # Calculate interpolation\n",
    "    alpha = torch.rand(b_size_gp, 1)\n",
    "    alpha = alpha.expand_as(real_data)\n",
    "    interpolated = (alpha * real_data.data + (1 - alpha) * generated_data.data).requires_grad_(True)\n",
    "\n",
    "    if ngpu > 0:\n",
    "        interpolated = interpolated.cuda()\n",
    "\n",
    "    # Calculate probability of interpolated examples\n",
    "    prob_interpolated = netD(interpolated)\n",
    "\n",
    "    # Calculate gradients of probabilities with respect to examples\n",
    "    gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                           grad_outputs=torch.ones(prob_interpolated.size()).cuda() if ngpu > 0 else torch.ones(\n",
    "                           prob_interpolated.size()),\n",
    "                           create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # Gradients have shape (b_size, num_channels, img_width, img_height),\n",
    "    # so flatten to easily take norm per example in batch\n",
    "    gradients = gradients.view(b_size_gp, -1)\n",
    "    # grad_norm = gradients.norm(2, dim=1).mean().item()\n",
    "\n",
    "    # Derivatives of the gradient close to 0 can cause problems because of\n",
    "    # the square root, so manually calculate norm and add epsilon\n",
    "    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "    avg_grad_norm = gradients_norm.mean().item()\n",
    "\n",
    "    # Return gradient penalty\n",
    "    return (gp_weight * ((gradients_norm - 1) ** 2).mean(), avg_grad_norm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Early stopping setup\n",
    "\n",
    "# Will have two backup points:\n",
    "# One where the sum of Wasserstein errors was minimal and one where the max was minimal\n",
    "\n",
    "min_sum = float('inf')\n",
    "min_sum_errors = [1.0 for i in range(a_dim)]\n",
    "min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "\n",
    "min_chen_err_sum = float('inf')\n",
    "min_chen_errors = [1.0 for i in range(a_dim)]\n",
    "min_chen_paramsG = copy.deepcopy(netG.state_dict())\n",
    "min_chen_paramsD = copy.deepcopy(netD.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/10, iter: 0, gradient norm: 0,\n",
      " errors: ['0.07738', '0.08373', '0.08979', '0.06425', '0.09158', '0.06308', '0.09172', '0.16757', '0.05835', '0.15810']\n",
      "Saved parameters (fixed error)\n",
      "epoch: 0/10, iter: 100, gradient norm: 0,\n",
      " errors: ['0.06918', '0.10300', '0.07590', '0.06136', '0.07828', '0.07233', '0.07571', '0.19159', '0.06371', '0.13831']\n",
      "Saved parameters (fixed error)\n",
      "epoch: 0/10, iter: 200, gradient norm: 0,\n",
      " errors: ['0.10799', '0.07341', '0.11909', '0.08782', '0.12206', '0.07693', '0.11938', '0.14203', '0.07816', '0.19238']\n",
      "epoch: 0/10, iter: 300, gradient norm: 0,\n",
      " errors: ['0.17977', '0.08723', '0.19544', '0.15737', '0.19617', '0.13275', '0.19563', '0.07480', '0.13937', '0.26997']\n",
      "epoch: 0/10, iter: 400, gradient norm: 0,\n",
      " errors: ['0.19922', '0.10239', '0.21418', '0.17664', '0.21877', '0.15240', '0.21791', '0.06586', '0.15726', '0.28960']\n",
      "epoch: 0/10, iter: 500, gradient norm: 0,\n",
      " errors: ['0.11341', '0.06466', '0.12422', '0.08985', '0.12688', '0.07554', '0.12636', '0.13081', '0.08080', '0.19786']\n",
      "epoch: 0/10, iter: 600, gradient norm: 0,\n",
      " errors: ['0.05921', '0.13268', '0.05967', '0.06821', '0.06105', '0.08960', '0.05779', '0.22875', '0.07940', '0.10313']\n",
      "epoch: 0/10, iter: 700, gradient norm: 0,\n",
      " errors: ['0.06187', '0.10559', '0.06747', '0.05756', '0.07022', '0.06905', '0.07052', '0.19616', '0.06213', '0.12971']\n",
      "Saved parameters (fixed error)\n",
      "epoch: 0/10, iter: 800, gradient norm: 0,\n",
      " errors: ['0.16186', '0.07499', '0.17404', '0.13846', '0.17842', '0.11364', '0.17709', '0.08721', '0.12016', '0.25047']\n",
      "epoch: 0/10, iter: 900, gradient norm: 0,\n",
      " errors: ['0.24583', '0.14942', '0.26262', '0.22113', '0.26320', '0.19440', '0.26510', '0.07548', '0.20342', '0.34007']\n",
      "epoch: 0/10, iter: 1000, gradient norm: 0,\n",
      " errors: ['0.20683', '0.11492', '0.22342', '0.18067', '0.22121', '0.15763', '0.22055', '0.07966', '0.16702', '0.29758']\n",
      "epoch: 0/10, iter: 1100, gradient norm: 0,\n",
      " errors: ['0.07684', '0.14779', '0.07566', '0.08531', '0.07710', '0.10751', '0.07131', '0.23790', '0.09554', '0.10754']\n",
      "epoch: 0/10, iter: 1200, gradient norm: 0,\n",
      " errors: ['0.12635', '0.22672', '0.11509', '0.14853', '0.11593', '0.17730', '0.11129', '0.32343', '0.16556', '0.07200']\n",
      "epoch: 0/10, iter: 1300, gradient norm: 0,\n",
      " errors: ['0.10108', '0.19352', '0.09258', '0.12092', '0.09373', '0.14787', '0.08854', '0.28916', '0.13404', '0.07526']\n",
      "epoch: 0/10, iter: 1400, gradient norm: 0,\n",
      " errors: ['0.12904', '0.07755', '0.14397', '0.10916', '0.14286', '0.09473', '0.14338', '0.12539', '0.09892', '0.21517']\n",
      "epoch: 0/10, iter: 1500, gradient norm: 0,\n",
      " errors: ['0.19489', '0.10626', '0.21040', '0.17280', '0.21242', '0.15027', '0.21233', '0.08158', '0.15966', '0.28466']\n",
      "epoch: 0/10, iter: 1600, gradient norm: 0,\n",
      " errors: ['0.07465', '0.14032', '0.07556', '0.08122', '0.07753', '0.09955', '0.07283', '0.22963', '0.09066', '0.11355']\n",
      "epoch: 0/10, iter: 1700, gradient norm: 0,\n",
      " errors: ['0.09447', '0.18591', '0.08760', '0.11244', '0.08959', '0.13787', '0.08323', '0.27947', '0.12708', '0.08072']\n",
      "epoch: 0/10, iter: 1800, gradient norm: 0,\n",
      " errors: ['0.07574', '0.14529', '0.07622', '0.08395', '0.07798', '0.10427', '0.07273', '0.23686', '0.09341', '0.11009']\n",
      "epoch: 0/10, iter: 1900, gradient norm: 0,\n",
      " errors: ['0.12913', '0.07791', '0.14229', '0.10860', '0.14361', '0.09590', '0.14629', '0.12612', '0.09829', '0.21449']\n",
      "epoch: 0/10, iter: 2000, gradient norm: 0,\n",
      " errors: ['0.14759', '0.08227', '0.16202', '0.12730', '0.16664', '0.11006', '0.16381', '0.11399', '0.11504', '0.23525']\n",
      "epoch: 1/10, iter: 2100, gradient norm: 0,\n",
      " errors: ['0.07979', '0.11985', '0.08487', '0.07673', '0.08861', '0.08820', '0.08416', '0.20305', '0.08010', '0.13892']\n",
      "epoch: 1/10, iter: 2200, gradient norm: 0,\n",
      " errors: ['0.08278', '0.15808', '0.07960', '0.09356', '0.08095', '0.11449', '0.07657', '0.25278', '0.10536', '0.10079']\n",
      "epoch: 1/10, iter: 2300, gradient norm: 0,\n",
      " errors: ['0.08093', '0.14686', '0.07938', '0.08566', '0.08160', '0.10809', '0.07845', '0.23257', '0.09525', '0.11468']\n",
      "epoch: 1/10, iter: 2400, gradient norm: 0,\n",
      " errors: ['0.11804', '0.09633', '0.13125', '0.10301', '0.13307', '0.09590', '0.12964', '0.16048', '0.09723', '0.19415']\n",
      "epoch: 1/10, iter: 2500, gradient norm: 0,\n",
      " errors: ['0.17020', '0.10107', '0.18302', '0.14727', '0.18226', '0.13157', '0.18170', '0.11595', '0.13680', '0.25255']\n",
      "epoch: 1/10, iter: 2600, gradient norm: 0,\n",
      " errors: ['0.10556', '0.09550', '0.11419', '0.08802', '0.11724', '0.08572', '0.11174', '0.16752', '0.08456', '0.17710']\n",
      "epoch: 1/10, iter: 2700, gradient norm: 0,\n",
      " errors: ['0.07917', '0.14920', '0.07747', '0.08685', '0.07931', '0.10905', '0.07440', '0.24144', '0.09866', '0.10768']\n",
      "epoch: 1/10, iter: 2800, gradient norm: 0,\n",
      " errors: ['0.07654', '0.14366', '0.07594', '0.08350', '0.07826', '0.10355', '0.07331', '0.23350', '0.09360', '0.11223']\n",
      "epoch: 1/10, iter: 2900, gradient norm: 0,\n",
      " errors: ['0.08728', '0.10070', '0.09541', '0.07604', '0.09748', '0.07941', '0.09651', '0.18470', '0.07443', '0.15832']\n",
      "epoch: 1/10, iter: 3000, gradient norm: 0,\n",
      " errors: ['0.15308', '0.08133', '0.16853', '0.13154', '0.17120', '0.11316', '0.16920', '0.10578', '0.11632', '0.24184']\n",
      "epoch: 1/10, iter: 3100, gradient norm: 0,\n",
      " errors: ['0.13942', '0.07781', '0.15483', '0.11795', '0.15384', '0.10082', '0.15720', '0.11940', '0.10626', '0.22832']\n",
      "epoch: 1/10, iter: 3200, gradient norm: 0,\n",
      " errors: ['0.08078', '0.11061', '0.08797', '0.07426', '0.08996', '0.08398', '0.08678', '0.19572', '0.07726', '0.14475']\n",
      "epoch: 1/10, iter: 3300, gradient norm: 0,\n",
      " errors: ['0.07744', '0.11677', '0.08398', '0.07403', '0.08573', '0.08542', '0.08286', '0.20299', '0.07902', '0.13707']\n",
      "epoch: 1/10, iter: 3400, gradient norm: 0,\n",
      " errors: ['0.09727', '0.08983', '0.10799', '0.08099', '0.10777', '0.07832', '0.11014', '0.16561', '0.07670', '0.17377']\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        netD.zero_grad()\n",
    "        netG.zero_grad()\n",
    "\n",
    "        # weight clipping so critic is lipschitz\n",
    "        for p in netD.parameters():\n",
    "            p.data.clamp_(-weight_cliping_limit, weight_cliping_limit)\n",
    "\n",
    "        # check actual batch size (last batch could be shorter)\n",
    "        b_size = data.size(0)\n",
    "\n",
    "        noise = torch.randn((b_size,noise_size), dtype=torch.float, device=device)\n",
    "        w = data[:,:w_dim]\n",
    "        z = torch.cat((noise,w), dim=1)\n",
    "        generated_A = netG(z)\n",
    "        fake_data = torch.cat((w,generated_A), dim=1)\n",
    "        fake_data = fake_data.detach()\n",
    "\n",
    "        # gradient_penalty, gradient_norm = _gradient_penalty(data,fake_data)\n",
    "\n",
    "        prob_real = netD(data)\n",
    "\n",
    "        prob_fake = netD(fake_data)\n",
    "\n",
    "        lossD_fake = prob_fake.mean(0).view(1)\n",
    "        lossD_real = prob_real.mean(0).view(1)\n",
    "        lossD = lossD_fake - lossD_real\n",
    "        lossD.backward()\n",
    "        optD.step()\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # train Generator with probability 1/5\n",
    "        if iters%20 == 0:\n",
    "            netG.zero_grad()\n",
    "            noise = torch.randn((b_size,noise_size), dtype=torch.float, device=device)\n",
    "            w = data[:,:w_dim]\n",
    "            z = torch.cat((noise,w), dim=1)\n",
    "            generated_A = netG(z)\n",
    "            fake_in = torch.cat((w,generated_A),dim=1)\n",
    "            lossG = netD(fake_in)\n",
    "            lossG = - lossG.mean(0).view(1)\n",
    "            lossG.backward()\n",
    "            optG.step()\n",
    "\n",
    "        if iters%100 == 0:\n",
    "            # Test Wasserstein error for fixed W\n",
    "            # noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "            # g_in = torch.cat((noise,W_fixed),1)\n",
    "            # A_fixed_gen = netG(g_in).detach().numpy()\n",
    "            # errors = [sqrt(ot.wasserstein_1d(A_fixed_true[:,i],A_fixed_gen[:,i],p=2)) for i in range(a_dim)]\n",
    "\n",
    "            errors = all_2dim_errors(netG)\n",
    "            pretty_errors = [\"{0:0.5f}\".format(i) for i in errors]\n",
    "\n",
    "            # Test Chen discrepancy\n",
    "            # W = torch.randn((4*batch_size, w_dim), dtype= torch.float, device=device)\n",
    "            # noise = torch.randn((4*batch_size,noise_size), dtype=torch.float, device=device)\n",
    "            # gen_in = torch.cat((noise,W),1)\n",
    "            # A_gen = netG(gen_in)\n",
    "            # w_a = torch.cat((W,A_gen.detach()),1)\n",
    "            # ch_err = chen_error_3step(w_a)\n",
    "            # pretty_chen_errors = [\"{0:0.5f}\".format(i) for i in ch_err]\n",
    "\n",
    "            # Print out partial results\n",
    "\n",
    "            print(f\"epoch: {epoch}/{num_epochs}, iter: {iters}, gradient norm: {0},\\n errors: {pretty_errors}\")\n",
    "            # Save for plotting\n",
    "            wass_errors.append(errors)\n",
    "            # chen_errors.append(ch_err)\n",
    "\n",
    "            # Early stopping checkpoint\n",
    "            error_sum = sum(errors)\n",
    "            if error_sum <= min_sum:\n",
    "                min_sum = error_sum\n",
    "                min_sum_errors = errors\n",
    "                min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "                min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "                print(\"Saved parameters (fixed error)\")\n",
    "\n",
    "            # chen_err_sum = sum(ch_err)\n",
    "            # if chen_err_sum < min_chen_err_sum:\n",
    "            #     min_chen_err_sum = chen_err_sum\n",
    "            #     min_chen_errors = ch_err\n",
    "            #     min_chen_paramsG = copy.deepcopy(netG.state_dict())\n",
    "            #     min_chen_paramsD = copy.deepcopy(netD.state_dict())\n",
    "            #     print(\"Saved parameters (chen errors)\")\n",
    "\n",
    "        iters += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "g_in = torch.cat((noise,W_fixed),1)\n",
    "A_fixed_gen = netG(g_in).detach()\n",
    "for o in A_fixed_gen:\n",
    "    print(o)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "W_fixed = [1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7]\n",
    "list_pairs(5) = [(1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)]\n",
    "\n",
    "GAN2 best: ['0.06348', '0.25953', '0.06187', '0.11594', '0.12005', '0.11992', '0.07918', '0.15956', '0.16242', '0.01383']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors: ['0.05866', '0.12589', '0.06246', '0.06572', '0.06344', '0.08378', '0.06210', '0.21660', '0.06989', '0.11190']\n"
     ]
    }
   ],
   "source": [
    "# Return to early stopping checkpoint\n",
    "if which_model == 1:\n",
    "    best_netG = Generator1().to(device)\n",
    "elif which_model == 2:\n",
    "    best_netG = Generator2().to(device)\n",
    "elif which_model == 3:\n",
    "    best_netG = Generator3().to(device)\n",
    "\n",
    "best_netG.load_state_dict(min_sum_paramsG)\n",
    "\n",
    "errors = all_2dim_errors(best_netG)\n",
    "pretty_errors = [\"{0:0.5f}\".format(i) for i in errors]\n",
    "print(f\"errors: {pretty_errors}\")\n",
    "\n",
    "torch.save(min_sum_paramsG, f'model_saves/GAN{which_model}_{w_dim}d_randomly_excellent_generator.pt')\n",
    "torch.save(min_sum_paramsD, f'model_saves/GAN{which_model}_{w_dim}d_randomly_excellent_discriminator.pt')\n",
    "# best_netD = Discriminator()\n",
    "# best_netD.load_state_dict(min_sum_paramsD)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test Wasserstein error for fixed W\n",
    "noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "g_in = torch.cat((noise,W_fixed),1)\n",
    "A_fixed_gen = best_netG(g_in).detach().numpy()\n",
    "errors = [sqrt(ot.wasserstein_1d(A_fixed_true[:,i],A_fixed_gen[:,i],p=2)) for i in range(a_dim)]\n",
    "\n",
    "# Print out partial results\n",
    "pretty_errors = [\"{0:0.5f}\".format(i) for i in errors]\n",
    "print(f\"best net errors: {pretty_errors}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "best net errors: ['0.06348', '0.25953', '0.06187', '0.11594', '0.12005', '0.11992', '0.07918', '0.15956', '0.16242', '0.01383']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw errors through iterations\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"2-Wasserstein distance of generated samples from the original samples for fixed W increment\")\n",
    "plt.plot(wass_errors)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Wasserstein distance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"2-Wasserstein distances after 2-step Chen recombinations\")\n",
    "plt.plot(chen_errors)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"2-Wasserstein distance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chen_iters = 0\n",
    "chen_training_wass_errors = []\n",
    "chen_training_chen_errors = []\n",
    "for i in range(num_Chen_iters):\n",
    "    netD.zero_grad()\n",
    "\n",
    "    # weight clipping so critic is lipschitz\n",
    "    for p in netD.parameters():\n",
    "        p.data.clamp_(-weight_cliping_limit, weight_cliping_limit)\n",
    "\n",
    "    # Train Discriminator\n",
    "    # generate 4*batch_size of fake data\n",
    "    W = torch.randn((4*batch_size, w_dim), dtype= torch.float, device=device)\n",
    "    noise = torch.randn((4*batch_size,noise_size), dtype=torch.float, device=device)\n",
    "    gen_in = torch.cat((noise,W),1)\n",
    "    A_gen = netG(gen_in)\n",
    "    fake_in = torch.cat((W,A_gen.detach()),1)\n",
    "    lossD_fake = netD(fake_in)\n",
    "    lossD_fake = lossD_fake.mean(0).view(1)\n",
    "    lossD_fake.backward(mone)\n",
    "\n",
    "    # now use chen_combine to produce \"true\" data from the fake one\n",
    "    # using chen_combine twice reduces batch dimension from 4*batch_size to batch_size\n",
    "    true_data = chen_combine(fake_in.detach())\n",
    "    true_data = chen_combine(true_data)\n",
    "    true_data = chen_combine(true_data)\n",
    "    assert true_data.size(0) == batch_size//2\n",
    "\n",
    "    lossD_real = netD(true_data)\n",
    "    lossD_real = 8 * lossD_real.mean(0).view(1) # multiply by 4 to counteract the 4x smaller batch\n",
    "    lossD_real.backward(one)\n",
    "    optD.step()\n",
    "\n",
    "    # train Generator with probability 1/5\n",
    "    # if np.random.randint(1,6) == 5:\n",
    "    if True:\n",
    "        netG.zero_grad()\n",
    "\n",
    "        fake_in = torch.cat((W,A_gen),1)\n",
    "        lossG = netD(fake_in)\n",
    "        lossG = lossG.mean(0).view(1)\n",
    "        lossG.backward(one)\n",
    "        optG.step()\n",
    "\n",
    "    if chen_iters%100 == 0:\n",
    "        # Test Wasserstein error for fixed W\n",
    "        noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "        g_in = torch.cat((noise,W_fixed),1)\n",
    "        A_fixed_gen = netG(g_in).detach().numpy()\n",
    "        errors = [sqrt(ot.wasserstein_1d(A_fixed_true[:,i],A_fixed_gen[:,i],p=2)) for i in range(a_dim)]\n",
    "        chen_training_wass_errors.append(errors)\n",
    "\n",
    "        # Test Chen discrepancy\n",
    "        W = torch.randn((4*batch_size, w_dim), dtype= torch.float, device=device)\n",
    "        noise = torch.randn((4*batch_size,noise_size), dtype=torch.float, device=device)\n",
    "        gen_in = torch.cat((noise,W),1)\n",
    "        A_gen = netG(gen_in)\n",
    "        w_a = torch.cat((W,A_gen.detach()),1)\n",
    "        ch_err = chen_error_3step(w_a)\n",
    "\n",
    "        # Print out partial results\n",
    "        pretty_errors = [\"{0:0.5f}\".format(i) for i in errors]\n",
    "        pretty_chen_errors = [\"{0:0.5f}\".format(i) for i in ch_err]\n",
    "        print(f\"iter: {chen_iters}/{num_Chen_iters},\\n errors: {pretty_errors}, \\n chen errors: {pretty_chen_errors}\")\n",
    "        # Save for plotting\n",
    "        chen_training_wass_errors.append(errors)\n",
    "        chen_training_chen_errors.append(ch_err)\n",
    "\n",
    "\n",
    "        # Early stopping checkpoint\n",
    "        error_sum = sum(errors)\n",
    "        if error_sum <= min_sum:\n",
    "            min_sum = error_sum\n",
    "            min_sum_errors = errors\n",
    "            min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "            min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "            print(\"Saved parameters\")\n",
    "\n",
    "    chen_iters += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw errors through iterations\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"2-Wasserstein distance of generated samples from the original samples for fixed W increment\")\n",
    "plt.plot(chen_training_wass_errors)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Wasserstein distance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Time measurements\n",
    "\n",
    "W_fixed: torch.Tensor = torch.tensor([1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7])\n",
    "W_fixed = W_fixed[:w_dim].unsqueeze(1).transpose(1,0)\n",
    "W_fixed = W_fixed.expand((test_batch_size,w_dim))\n",
    "noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "g_in = torch.cat((noise,W_fixed),1)\n",
    "netG.eval()\n",
    "start_time = timeit.default_timer()\n",
    "for i in range(100):\n",
    "    A_fixed_out=netG(g_in)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Takes 34.2s to generate 6553600 samples (original GAN)\n",
    "Calling iterated_integrals(h = 1.0, err = 0.0005) 6553600-times takes 100.5s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}