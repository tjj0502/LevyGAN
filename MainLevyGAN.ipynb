{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import ot\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import copy\n",
    "from math import sqrt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "noise_size = 62\n",
    "\n",
    "# Number of training epochs using classical training\n",
    "num_epochs = 4\n",
    "\n",
    "# Number of iterations of Chen training\n",
    "num_Chen_iters = 5000\n",
    "\n",
    "# 'Adam' of 'RMSProp'\n",
    "which_optimizer = 'Adam'\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0001\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "ngpu = 0\n",
    "\n",
    "# To keep the criterion Lipschitz\n",
    "weight_cliping_limit = 0.01\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "test_batch_size = 65536\n",
    "\n",
    "w_dim = 5\n",
    "\n",
    "a_dim = int(w_dim*(w_dim - 1)//2)\n",
    "\n",
    "# if 1 use GAN1, if 2 use GAN2, etc.\n",
    "which_model = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CHEN RELATION\n",
    "\n",
    "def chen_combine(w_a_in: torch.TensorType):\n",
    "    # the batch dimension of the inputs will be quartered\n",
    "    out_size = w_a_in.size(0)//2\n",
    "    assert 2*out_size == w_a_in.size(0)\n",
    "    assert w_a_in.size(1) == w_dim + a_dim\n",
    "\n",
    "    # w_0_s is from 0 to t/2 and w_s_t is from t/2 to t\n",
    "    w_0_s,w_s_t = w_a_in.chunk(2)\n",
    "    result = torch.clone(w_0_s + w_s_t)\n",
    "    result[:,:w_dim] = sqrt(0.5)*result[:,:w_dim]\n",
    "    result[:,w_dim:(w_dim+a_dim)] = 0.5*result[:,w_dim:(w_dim+a_dim)]\n",
    "\n",
    "    idx = w_dim\n",
    "    for k in range(w_dim - 1):\n",
    "        for l in range(k+1,w_dim):\n",
    "            correction_term = 0.25*(w_0_s[:,k]*w_s_t[:,l] - w_0_s[:,l]*w_s_t[:,k])\n",
    "            result[:,idx] += correction_term\n",
    "            idx += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "# prints the 2-Wasserstein distances (in each of the Levy-area dimensions) between the input and chen_combine(chen_combine(input))\n",
    "\n",
    "def chen_error_2step(w_a_in: torch.TensorType):\n",
    "    combined_data = chen_combine(w_a_in)\n",
    "    combined_data = chen_combine(combined_data)\n",
    "    return [sqrt(ot.wasserstein_1d(combined_data[:,w_dim+i],w_a_in[:,w_dim+i],p=2)) for i in range(a_dim)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# create dataloader for samples\n",
    "\n",
    "def row_processer(row):\n",
    "    return np.array(row, dtype= np.float32)\n",
    "\n",
    "filename = f\"samples/samples_{w_dim}-dim.csv\"\n",
    "datapipe = dp.iter.FileOpener([filename], mode='b')\n",
    "datapipe = datapipe.parse_csv(delimiter=',')\n",
    "datapipe = datapipe.map(row_processer)\n",
    "dataloader = DataLoader(dataset=datapipe, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "# Check if the dimensions match\n",
    "d = next(iter(dataloader))\n",
    "if d.size(1) != a_dim + w_dim:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!! WRONG DATA DIMENSIONS !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    if classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# GAN 1\n",
    "\n",
    "class Generator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim+noise_size,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# GAN 2\n",
    "\n",
    "class Generator2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim+noise_size,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512,a_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_dim + a_dim,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            nn.Linear(1024,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            nn.Linear(256,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "Generator(\n  (main): Sequential(\n    (0): Linear(in_features=67, out_features=1024, bias=True)\n    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Linear(in_features=1024, out_features=1024, bias=True)\n    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU()\n    (6): Linear(in_features=1024, out_features=512, bias=True)\n    (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU()\n    (9): Linear(in_features=512, out_features=10, bias=True)\n  )\n)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize nets\n",
    "if which_model == 1:\n",
    "    netD = Discriminator1().to(device)\n",
    "    netG = Generator1().to(device)\n",
    "elif which_model == 2:\n",
    "    netD = Discriminator2().to(device)\n",
    "    netG = Generator2().to(device)\n",
    "\n",
    "\n",
    "netD.apply(weights_init)\n",
    "netG.apply(weights_init)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Initialise optimiser\n",
    "\n",
    "if which_optimizer == 'Adam':\n",
    "    optG = torch.optim.Adam(netG.parameters(),lr = lr, betas=(beta1,0.999))\n",
    "    optD = torch.optim.Adam(netD.parameters(), lr = lr, betas=(beta1,0.999))\n",
    "elif which_optimizer == 'RMSProp':\n",
    "    optG = torch.optim.RMSprop(netG.parameters(), lr = lr)\n",
    "    optD = torch.optim.RMSprop(netD.parameters(), lr = lr)\n",
    "\n",
    "# A fixed W increment for testing purposes\n",
    "W_fixed: torch.Tensor = torch.tensor([1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7])\n",
    "\n",
    "\n",
    "W_fixed = W_fixed[:w_dim].unsqueeze(1).transpose(1,0)\n",
    "W_fixed = W_fixed.expand((test_batch_size,w_dim))\n",
    "\n",
    "# Load \"true\" samples generated from this fixed W increment\n",
    "test_filename = f\"samples/fixed_samples_{w_dim}-dim.csv\"\n",
    "A_fixed_true = np.genfromtxt(test_filename,dtype=float,delimiter=',',)\n",
    "A_fixed_true = A_fixed_true[:,w_dim:(w_dim+a_dim)]\n",
    "\n",
    "wass_errors = []\n",
    "chen_errors = []\n",
    "\n",
    "iters = 0\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Early stopping setup\n",
    "\n",
    "# Will have two backup points:\n",
    "# One where the sum of Wasserstein errors was minimal and one where the max was minimal\n",
    "\n",
    "min_sum = float('inf')\n",
    "min_sum_errors = [1.0 for i in range(a_dim)]\n",
    "min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "\n",
    "# min_max_err = float('inf')\n",
    "# min_max_errors = [1.0 for i in range(a_dim)]\n",
    "# min_max_paramsG = copy.deepcopy(netG.state_dict())\n",
    "# min_max_paramsD = copy.deepcopy(netD.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/15, iter: 0,\n",
      " errors: ['0.14270', '0.22039', '0.08434', '0.12691', '0.24892', '0.15976', '0.14762', '0.17953', '0.14869', '0.37229']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 100,\n",
      " errors: ['0.12614', '0.11775', '0.13972', '0.05907', '0.24498', '0.25003', '0.20662', '0.16803', '0.05903', '0.39907']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 200,\n",
      " errors: ['0.11545', '0.17457', '0.18115', '0.17742', '0.21195', '0.25333', '0.21748', '0.19232', '0.12961', '0.33502']\n",
      "epoch: 0/15, iter: 300,\n",
      " errors: ['0.12090', '0.09632', '0.16946', '0.08668', '0.26374', '0.23058', '0.21729', '0.27858', '0.09916', '0.37253']\n",
      "epoch: 0/15, iter: 400,\n",
      " errors: ['0.15521', '0.10686', '0.11399', '0.12052', '0.26080', '0.18956', '0.17489', '0.18138', '0.03185', '0.36880']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 500,\n",
      " errors: ['0.11224', '0.12227', '0.12311', '0.06955', '0.23459', '0.19410', '0.17166', '0.19517', '0.03523', '0.37004']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 600,\n",
      " errors: ['0.13447', '0.11075', '0.08419', '0.05007', '0.20614', '0.25502', '0.18341', '0.14912', '0.02874', '0.39464']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 700,\n",
      " errors: ['0.11569', '0.14288', '0.09268', '0.06006', '0.22435', '0.23524', '0.14921', '0.17446', '0.03825', '0.39130']\n",
      "epoch: 0/15, iter: 800,\n",
      " errors: ['0.13972', '0.12175', '0.11996', '0.06505', '0.22787', '0.23260', '0.11470', '0.18953', '0.04731', '0.37659']\n",
      "epoch: 0/15, iter: 900,\n",
      " errors: ['0.11580', '0.13419', '0.14150', '0.10343', '0.25081', '0.16919', '0.12730', '0.17289', '0.04104', '0.32270']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 1000,\n",
      " errors: ['0.13180', '0.10812', '0.07402', '0.04341', '0.24490', '0.24603', '0.14034', '0.19763', '0.05157', '0.34819']\n",
      "epoch: 0/15, iter: 1100,\n",
      " errors: ['0.12439', '0.13143', '0.08376', '0.04345', '0.23323', '0.21253', '0.18949', '0.17746', '0.05583', '0.35924']\n",
      "epoch: 0/15, iter: 1200,\n",
      " errors: ['0.14412', '0.09839', '0.10416', '0.03885', '0.23165', '0.20468', '0.10020', '0.16024', '0.05700', '0.34990']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 1300,\n",
      " errors: ['0.14045', '0.08996', '0.04130', '0.05878', '0.25213', '0.22151', '0.20388', '0.18422', '0.04799', '0.32519']\n",
      "epoch: 0/15, iter: 1400,\n",
      " errors: ['0.08288', '0.08153', '0.03757', '0.07798', '0.24320', '0.17335', '0.18550', '0.21589', '0.05060', '0.29569']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 1500,\n",
      " errors: ['0.13216', '0.13986', '0.03712', '0.03056', '0.22883', '0.15400', '0.15370', '0.16458', '0.04037', '0.31853']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 1600,\n",
      " errors: ['0.10150', '0.08242', '0.06163', '0.04719', '0.22535', '0.15533', '0.11628', '0.15633', '0.03794', '0.31038']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 1700,\n",
      " errors: ['0.12283', '0.11405', '0.04555', '0.04661', '0.19999', '0.19855', '0.11034', '0.11588', '0.04197', '0.32641']\n",
      "epoch: 0/15, iter: 1800,\n",
      " errors: ['0.09130', '0.09768', '0.03206', '0.04390', '0.22943', '0.17021', '0.13889', '0.11915', '0.03287', '0.32198']\n",
      "Saved parameters\n",
      "epoch: 0/15, iter: 1900,\n",
      " errors: ['0.14416', '0.12225', '0.05095', '0.04327', '0.18401', '0.16276', '0.13458', '0.17915', '0.06421', '0.32536']\n",
      "epoch: 0/15, iter: 2000,\n",
      " errors: ['0.08961', '0.09810', '0.05003', '0.03691', '0.23118', '0.21802', '0.10633', '0.16303', '0.06815', '0.29605']\n",
      "epoch: 1/15, iter: 2100,\n",
      " errors: ['0.10649', '0.11797', '0.07041', '0.05415', '0.22769', '0.17607', '0.12241', '0.14266', '0.07436', '0.35113']\n",
      "epoch: 1/15, iter: 2200,\n",
      " errors: ['0.08609', '0.09874', '0.06204', '0.06340', '0.24855', '0.15225', '0.09743', '0.13854', '0.07087', '0.35355']\n",
      "epoch: 1/15, iter: 2300,\n",
      " errors: ['0.07957', '0.17263', '0.07023', '0.02974', '0.16377', '0.16634', '0.10408', '0.12315', '0.06355', '0.29440']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 2400,\n",
      " errors: ['0.09219', '0.18268', '0.03876', '0.06029', '0.15284', '0.19297', '0.08503', '0.13674', '0.06462', '0.29383']\n",
      "epoch: 1/15, iter: 2500,\n",
      " errors: ['0.12983', '0.17441', '0.04671', '0.06239', '0.16443', '0.16010', '0.06518', '0.13170', '0.09067', '0.28127']\n",
      "epoch: 1/15, iter: 2600,\n",
      " errors: ['0.11930', '0.19088', '0.03580', '0.06736', '0.17578', '0.17887', '0.11549', '0.12549', '0.08376', '0.28912']\n",
      "epoch: 1/15, iter: 2700,\n",
      " errors: ['0.09450', '0.16291', '0.03109', '0.05464', '0.20445', '0.16608', '0.08415', '0.12500', '0.07165', '0.28202']\n",
      "epoch: 1/15, iter: 2800,\n",
      " errors: ['0.08862', '0.17731', '0.03726', '0.07002', '0.17389', '0.14278', '0.05011', '0.12561', '0.14272', '0.25358']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 2900,\n",
      " errors: ['0.08552', '0.20450', '0.04642', '0.07033', '0.19375', '0.14553', '0.10061', '0.08479', '0.12671', '0.24882']\n",
      "epoch: 1/15, iter: 3000,\n",
      " errors: ['0.12991', '0.18476', '0.08122', '0.06180', '0.15871', '0.14081', '0.02782', '0.13593', '0.12610', '0.24456']\n",
      "epoch: 1/15, iter: 3100,\n",
      " errors: ['0.08653', '0.20307', '0.07232', '0.08002', '0.17297', '0.15334', '0.04061', '0.11976', '0.14565', '0.25994']\n",
      "epoch: 1/15, iter: 3200,\n",
      " errors: ['0.05289', '0.19248', '0.08452', '0.09116', '0.14548', '0.14252', '0.02354', '0.10597', '0.16665', '0.22692']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 3300,\n",
      " errors: ['0.06560', '0.21113', '0.05726', '0.11028', '0.18361', '0.09060', '0.02720', '0.11852', '0.16155', '0.22849']\n",
      "epoch: 1/15, iter: 3400,\n",
      " errors: ['0.07327', '0.21255', '0.03599', '0.12105', '0.16532', '0.12156', '0.01973', '0.12149', '0.18184', '0.19919']\n",
      "epoch: 1/15, iter: 3500,\n",
      " errors: ['0.06415', '0.20352', '0.06339', '0.12892', '0.14830', '0.09145', '0.02878', '0.13317', '0.17926', '0.18468']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 3600,\n",
      " errors: ['0.11777', '0.22269', '0.07403', '0.12146', '0.13133', '0.05396', '0.06353', '0.11505', '0.17549', '0.20477']\n",
      "epoch: 1/15, iter: 3700,\n",
      " errors: ['0.04049', '0.21104', '0.10482', '0.11583', '0.15695', '0.04858', '0.06267', '0.12763', '0.12471', '0.19401']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 3800,\n",
      " errors: ['0.07686', '0.19928', '0.07781', '0.09002', '0.16473', '0.07216', '0.05091', '0.11405', '0.13920', '0.17736']\n",
      "Saved parameters\n",
      "epoch: 1/15, iter: 3900,\n",
      " errors: ['0.11196', '0.24698', '0.04899', '0.14479', '0.16888', '0.06462', '0.10943', '0.08882', '0.18711', '0.21317']\n",
      "epoch: 1/15, iter: 4000,\n",
      " errors: ['0.08378', '0.25426', '0.12137', '0.11381', '0.12346', '0.07059', '0.04567', '0.12183', '0.17306', '0.13079']\n",
      "epoch: 2/15, iter: 4100,\n",
      " errors: ['0.06848', '0.23641', '0.07475', '0.11082', '0.15898', '0.01844', '0.03814', '0.11156', '0.16812', '0.13951']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 4200,\n",
      " errors: ['0.09289', '0.16066', '0.09156', '0.11174', '0.18290', '0.05310', '0.04811', '0.14275', '0.14953', '0.16176']\n",
      "epoch: 2/15, iter: 4300,\n",
      " errors: ['0.06014', '0.26200', '0.05887', '0.11081', '0.16086', '0.05078', '0.08597', '0.15666', '0.15192', '0.28886']\n",
      "epoch: 2/15, iter: 4400,\n",
      " errors: ['0.06379', '0.20660', '0.06336', '0.09737', '0.17123', '0.08722', '0.05740', '0.20591', '0.15407', '0.31063']\n",
      "epoch: 2/15, iter: 4500,\n",
      " errors: ['0.04864', '0.13841', '0.11944', '0.10842', '0.20472', '0.02800', '0.05463', '0.13611', '0.16958', '0.29731']\n",
      "epoch: 2/15, iter: 4600,\n",
      " errors: ['0.08881', '0.12086', '0.22126', '0.17504', '0.15217', '0.15266', '0.04514', '0.07295', '0.12600', '0.35511']\n",
      "epoch: 2/15, iter: 4700,\n",
      " errors: ['0.10353', '0.16136', '0.07835', '0.11298', '0.15524', '0.02970', '0.03447', '0.07909', '0.15747', '0.21936']\n",
      "epoch: 2/15, iter: 4800,\n",
      " errors: ['0.08721', '0.26020', '0.06333', '0.15363', '0.16582', '0.01984', '0.05206', '0.10207', '0.16105', '0.14159']\n",
      "epoch: 2/15, iter: 4900,\n",
      " errors: ['0.10292', '0.23345', '0.08013', '0.13373', '0.17653', '0.07591', '0.06082', '0.10302', '0.12734', '0.18096']\n",
      "epoch: 2/15, iter: 5000,\n",
      " errors: ['0.12313', '0.15866', '0.14553', '0.11657', '0.19559', '0.08282', '0.06586', '0.11549', '0.13350', '0.25640']\n",
      "epoch: 2/15, iter: 5100,\n",
      " errors: ['0.06394', '0.16886', '0.09038', '0.12174', '0.13994', '0.09257', '0.04650', '0.18511', '0.12272', '0.24232']\n",
      "epoch: 2/15, iter: 5200,\n",
      " errors: ['0.08438', '0.19562', '0.06449', '0.10934', '0.17132', '0.04827', '0.03409', '0.11116', '0.14675', '0.24199']\n",
      "epoch: 2/15, iter: 5300,\n",
      " errors: ['0.11976', '0.20057', '0.10452', '0.11161', '0.16050', '0.03513', '0.04217', '0.15536', '0.14751', '0.20128']\n",
      "epoch: 2/15, iter: 5400,\n",
      " errors: ['0.10534', '0.19926', '0.13759', '0.11340', '0.14973', '0.06605', '0.04083', '0.13482', '0.15237', '0.18392']\n",
      "epoch: 2/15, iter: 5500,\n",
      " errors: ['0.11018', '0.20320', '0.09608', '0.10369', '0.15219', '0.05964', '0.03068', '0.12000', '0.15157', '0.19207']\n",
      "epoch: 2/15, iter: 5600,\n",
      " errors: ['0.08932', '0.18285', '0.07975', '0.10431', '0.14316', '0.04109', '0.02709', '0.13512', '0.15264', '0.17444']\n",
      "epoch: 2/15, iter: 5700,\n",
      " errors: ['0.07147', '0.19484', '0.08978', '0.11066', '0.13839', '0.04682', '0.03648', '0.13795', '0.15062', '0.18903']\n",
      "epoch: 2/15, iter: 5800,\n",
      " errors: ['0.09773', '0.18527', '0.08150', '0.10222', '0.12243', '0.03812', '0.03848', '0.14387', '0.15069', '0.19801']\n",
      "epoch: 2/15, iter: 5900,\n",
      " errors: ['0.13249', '0.18794', '0.06546', '0.06694', '0.13879', '0.02518', '0.04047', '0.12900', '0.15097', '0.17740']\n",
      "Saved parameters\n",
      "epoch: 2/15, iter: 6000,\n",
      " errors: ['0.12531', '0.21664', '0.12229', '0.08961', '0.11903', '0.04126', '0.01676', '0.13800', '0.15268', '0.12370']\n",
      "epoch: 2/15, iter: 6100,\n",
      " errors: ['0.10542', '0.16748', '0.11707', '0.09212', '0.14963', '0.07522', '0.02321', '0.11563', '0.16498', '0.18851']\n",
      "epoch: 3/15, iter: 6200,\n",
      " errors: ['0.13071', '0.17610', '0.11571', '0.08630', '0.13965', '0.05838', '0.01094', '0.15422', '0.14859', '0.17165']\n",
      "epoch: 3/15, iter: 6300,\n",
      " errors: ['0.13878', '0.19008', '0.10845', '0.14647', '0.13441', '0.03579', '0.02962', '0.12379', '0.09972', '0.16974']\n",
      "epoch: 3/15, iter: 6400,\n",
      " errors: ['0.08978', '0.19671', '0.08749', '0.08899', '0.08980', '0.06111', '0.03800', '0.15452', '0.11098', '0.15033']\n",
      "Saved parameters\n",
      "epoch: 3/15, iter: 6500,\n",
      " errors: ['0.11467', '0.19501', '0.10969', '0.09256', '0.11896', '0.03464', '0.03537', '0.15443', '0.14190', '0.14523']\n",
      "epoch: 3/15, iter: 6600,\n",
      " errors: ['0.06860', '0.20791', '0.05636', '0.11171', '0.12932', '0.05532', '0.02169', '0.15102', '0.13399', '0.22059']\n",
      "epoch: 3/15, iter: 6700,\n",
      " errors: ['0.12040', '0.19754', '0.09104', '0.09153', '0.10054', '0.05266', '0.02361', '0.12940', '0.14341', '0.21628']\n",
      "epoch: 3/15, iter: 6800,\n",
      " errors: ['0.10503', '0.16846', '0.11701', '0.11496', '0.10442', '0.06034', '0.02907', '0.17588', '0.15911', '0.24105']\n",
      "epoch: 3/15, iter: 6900,\n",
      " errors: ['0.12284', '0.18133', '0.12107', '0.09666', '0.08844', '0.05943', '0.01237', '0.14204', '0.12832', '0.19638']\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # weight clipping so critic is lipschitz\n",
    "        for p in netD.parameters():\n",
    "            p.data.clamp_(-weight_cliping_limit, weight_cliping_limit)\n",
    "\n",
    "        # check actual batch size (last batch could be shorter)\n",
    "        b_size = data.size(0)\n",
    "\n",
    "        # Train Discriminator\n",
    "        # first on real data\n",
    "        out_D_real = netD(data)\n",
    "        lossDr = out_D_real.mean(0).view(1)\n",
    "        lossDr.backward(one)\n",
    "\n",
    "        # then on fake data\n",
    "\n",
    "        # data has shape (b_size, w_dim + a_dim) where w_dim are the dimensions of the driving BM and a_dim is the dim of Levy Areas\n",
    "        W = data[:,:w_dim]\n",
    "        A_real = data[:,w_dim:(w_dim + a_dim)]\n",
    "        noise = torch.randn((b_size,noise_size), dtype=torch.float, device=device)\n",
    "        gen_in = torch.cat((noise,W),1)\n",
    "        # generate fake data\n",
    "        generated_A = netG(gen_in)\n",
    "        fake_in = torch.cat((W,generated_A.detach()),1)\n",
    "\n",
    "        lossDf = netD(fake_in)\n",
    "        lossDf = lossDf.mean(0).view(1)\n",
    "        lossDf.backward(mone)\n",
    "        lossD = lossDr - lossDf\n",
    "        optD.step()\n",
    "\n",
    "        # train Generator with probability 1/5\n",
    "        if np.random.randint(1,6) == 5:\n",
    "            netG.zero_grad()\n",
    "\n",
    "            fake_in = torch.cat((W,generated_A),1)\n",
    "            lossG = netD(fake_in)\n",
    "            lossG = lossG.mean(0).view(1)\n",
    "            lossG.backward(one)\n",
    "            optG.step()\n",
    "\n",
    "        if iters%100 == 0:\n",
    "            # Test Wasserstein error for fixed W\n",
    "            noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "            g_in = torch.cat((noise,W_fixed),1)\n",
    "            A_fixed_gen = netG(g_in).detach().numpy()\n",
    "            errors = [sqrt(ot.wasserstein_1d(A_fixed_true[:,i],A_fixed_gen[:,i],p=2)) for i in range(a_dim)]\n",
    "\n",
    "            # Test Chen discrepancy\n",
    "            W = torch.randn((4*batch_size, w_dim), dtype= torch.float, device=device)\n",
    "            noise = torch.randn((4*batch_size,noise_size), dtype=torch.float, device=device)\n",
    "            gen_in = torch.cat((noise,W),1)\n",
    "            A_gen = netG(gen_in)\n",
    "            w_a = torch.cat((W,A_gen.detach()),1)\n",
    "            ch_err = chen_error_2step(w_a)\n",
    "\n",
    "            # Print out partial results\n",
    "            pretty_errors = [\"{0:0.5f}\".format(i) for i in errors]\n",
    "            pretty_chen_errors = [\"{0:0.5f}\".format(i) for i in ch_err]\n",
    "            print(f\"epoch: {epoch}/{num_epochs}, iter: {iters},\\n errors: {pretty_errors}, \\n chen errors: {pretty_chen_errors}\")\n",
    "            # Save for plotting\n",
    "            wass_errors.append(errors)\n",
    "            chen_errors.append(ch_err)\n",
    "\n",
    "            # Early stopping checkpoint\n",
    "            error_sum = sum(errors)\n",
    "            if error_sum <= min_sum:\n",
    "                min_sum = error_sum\n",
    "                min_sum_errors = errors\n",
    "                min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "                min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "                print(\"Saved parameters\")\n",
    "\n",
    "        iters += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "W_fixed = [1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7]\n",
    "list_pairs(5) = [(1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)]\n",
    "\n",
    "GAN2 best: ['0.06348', '0.25953', '0.06187', '0.11594', '0.12005', '0.11992', '0.07918', '0.15956', '0.16242', '0.01383']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Return to early stopping checkpoint\n",
    "if which_model == 1:\n",
    "    best_netG = Generator1().to(device)\n",
    "elif which_model == 2:\n",
    "    best_netG = Generator2().to(device)\n",
    "\n",
    "best_netG.load_state_dict(min_sum_paramsG)\n",
    "\n",
    "torch.save(min_sum_paramsG, f'model_saves/GAN2_{w_dim}d_24epochs_generator.pt')\n",
    "torch.save(min_sum_paramsD, f'model_saves/GAN2_{w_dim}d_24epochs_discriminator.pt')\n",
    "# best_netD = Discriminator()\n",
    "# best_netD.load_state_dict(min_sum_paramsD)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best net errors: ['0.06348', '0.25953', '0.06187', '0.11594', '0.12005', '0.11992', '0.07918', '0.15956', '0.16242', '0.01383']\n"
     ]
    }
   ],
   "source": [
    "# Test Wasserstein error for fixed W\n",
    "noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "g_in = torch.cat((noise,W_fixed),1)\n",
    "A_fixed_gen = best_netG(g_in).detach().numpy()\n",
    "errors = [sqrt(ot.wasserstein_1d(A_fixed_true[:,i],A_fixed_gen[:,i],p=2)) for i in range(a_dim)]\n",
    "\n",
    "# Print out partial results\n",
    "pretty_errors = [\"{0:0.5f}\".format(i) for i in errors]\n",
    "print(f\"best net errors: {pretty_errors}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "best net errors: ['0.06348', '0.25953', '0.06187', '0.11594', '0.12005', '0.11992', '0.07918', '0.15956', '0.16242', '0.01383']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw errors through iterations\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"2-Wasserstein distance of generated samples from the original samples for fixed W increment\")\n",
    "plt.plot(wass_errors)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Wasserstein distance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"2-Wasserstein distances after 2-step Chen recombinations\")\n",
    "plt.plot(chen_errors)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"2-Wasserstein distance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chen_iters = 0\n",
    "chen_training_wass_errors = []\n",
    "chen_training_chen_errors = []\n",
    "for i in range(num_Chen_iters):\n",
    "    netD.zero_grad()\n",
    "\n",
    "    # weight clipping so critic is lipschitz\n",
    "    for p in netD.parameters():\n",
    "        p.data.clamp_(-weight_cliping_limit, weight_cliping_limit)\n",
    "\n",
    "    # Train Discriminator\n",
    "    # generate 4*batch_size of fake data\n",
    "    W = torch.randn((4*batch_size, w_dim), dtype= torch.float, device=device)\n",
    "    noise = torch.randn((4*batch_size,noise_size), dtype=torch.float, device=device)\n",
    "    gen_in = torch.cat((noise,W),1)\n",
    "    A_gen = netG(gen_in)\n",
    "    fake_in = torch.cat((W,A_gen.detach()),1)\n",
    "    lossD_fake = netD(fake_in)\n",
    "    lossD_fake = lossD_fake.mean(0).view(1)\n",
    "    lossD_fake.backward(mone)\n",
    "\n",
    "    # now use chen_combine to produce \"true\" data from the fake one\n",
    "    # using chen_combine twice reduces batch dimension from 4*batch_size to batch_size\n",
    "    true_data = chen_combine(fake_in.detach())\n",
    "    true_data = chen_combine(true_data)\n",
    "    assert true_data.size(0) == batch_size\n",
    "\n",
    "    lossD_real = netD(true_data)\n",
    "    lossD_real = 4 * lossD_real.mean(0).view(1) # multiply by 4 to counteract the 4x smaller batch\n",
    "    lossD_real.backward(one)\n",
    "    optD.step()\n",
    "\n",
    "    # train Generator with probability 1/5\n",
    "    if np.random.randint(1,6) == 5:\n",
    "        netG.zero_grad()\n",
    "\n",
    "        fake_in = torch.cat((W,A_gen),1)\n",
    "        lossG = netD(fake_in)\n",
    "        lossG = lossG.mean(0).view(1)\n",
    "        lossG.backward(one)\n",
    "        optG.step()\n",
    "\n",
    "    if chen_iters%100 == 0:\n",
    "        # Test Wasserstein error for fixed W\n",
    "        noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "        g_in = torch.cat((noise,W_fixed),1)\n",
    "        A_fixed_gen = netG(g_in).detach().numpy()\n",
    "        errors = [sqrt(ot.wasserstein_1d(A_fixed_true[:,i],A_fixed_gen[:,i],p=2)) for i in range(a_dim)]\n",
    "        chen_training_wass_errors.append(errors)\n",
    "\n",
    "        # Test Chen discrepancy\n",
    "        W = torch.randn((4*batch_size, w_dim), dtype= torch.float, device=device)\n",
    "        noise = torch.randn((4*batch_size,noise_size), dtype=torch.float, device=device)\n",
    "        gen_in = torch.cat((noise,W),1)\n",
    "        A_gen = netG(gen_in)\n",
    "        w_a = torch.cat((W,A_gen.detach()),1)\n",
    "        ch_err = chen_error_2step(w_a)\n",
    "\n",
    "        # Print out partial results\n",
    "        pretty_errors = [\"{0:0.5f}\".format(i) for i in errors]\n",
    "        pretty_chen_errors = [\"{0:0.5f}\".format(i) for i in ch_err]\n",
    "        print(f\"iter: {chen_iters}/{num_Chen_iters},\\n errors: {pretty_errors}, \\n chen errors: {pretty_chen_errors}\")\n",
    "        # Save for plotting\n",
    "        chen_training_wass_errors.append(errors)\n",
    "        chen_training_chen_errors.append(ch_err)\n",
    "\n",
    "\n",
    "        # Early stopping checkpoint\n",
    "        error_sum = sum(errors)\n",
    "        if error_sum <= min_sum:\n",
    "            min_sum = error_sum\n",
    "            min_sum_errors = errors\n",
    "            min_sum_paramsG = copy.deepcopy(netG.state_dict())\n",
    "            min_sum_paramsD = copy.deepcopy(netD.state_dict())\n",
    "            print(\"Saved parameters\")\n",
    "\n",
    "    chen_iters += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Time measurements\n",
    "\n",
    "W_fixed: torch.Tensor = torch.tensor([1.0,-0.5,-1.2,-0.3,0.7,0.2,-0.9,0.1,1.7])\n",
    "W_fixed = W_fixed[:w_dim].unsqueeze(1).transpose(1,0)\n",
    "W_fixed = W_fixed.expand((test_batch_size,w_dim))\n",
    "noise = torch.randn((test_batch_size,noise_size), dtype=torch.float, device=device)\n",
    "g_in = torch.cat((noise,W_fixed),1)\n",
    "netG.eval()\n",
    "start_time = timeit.default_timer()\n",
    "for i in range(100):\n",
    "    A_fixed_out=netG(g_in)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Takes 34.2s to generate 6553600 samples (original GAN)\n",
    "Calling iterated_integrals(h = 1.0, err = 0.0005) 6553600-times takes 100.5s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}